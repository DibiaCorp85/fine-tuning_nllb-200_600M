{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DibiaCorp85/fine-tuning_nllb-200_600M/blob/main/_Fine_Tuning_En_Ha_LaTn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46HOKOHDsASy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMTxEDN6sEjk"
      },
      "source": [
        "## **Install Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrAh2JE3sEf8"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet chainlit pyngrok datasets transformers evaluate accelerate peft sacrebleu rouge_score bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP3dw5L8D204"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade fsspec datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y9wq391sEdk"
      },
      "source": [
        "## **Import Core Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7ir1BGVsEa_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from datasets import (load_dataset,\n",
        "                      concatenate_datasets,\n",
        "                      DatasetDict,\n",
        "                      Dataset,\n",
        "                      get_dataset_config_names,\n",
        "                      Features,\n",
        "                      ClassLabel,\n",
        "                      Value,\n",
        "                      Translation\n",
        "                      )\n",
        "\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        ")\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "import getpass\n",
        "from pyngrok import conf, ngrok\n",
        "import subprocess\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB89k0d3sEXI",
        "outputId": "311ccbe0-3fb2-4c1a-d159-c7090330a756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to save model and logs\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ha_LaTn\"\n",
        "os.makedirs(save_dir, exist_ok = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGPT4cP7sEUt"
      },
      "source": [
        "## **Load Datasets**\n",
        "\n",
        "English-\"Language\" datasets are loaded from Hugging Face.\n",
        "\n",
        "For this process, the following language is considered:\n",
        "* Hausa\n",
        "\n",
        "The following are the datasets are used:\n",
        "\n",
        "* Opus100 containing the above listed languages paired with English language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wxoGrp5SpN"
      },
      "source": [
        "### **Opus100 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC2sCb91sERW",
        "outputId": "51cb09c3-5295-447f-eca2-72ff79b595a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The English-Hausa language pair is present in Opus100 dataset:\n",
            " - en-ha (Hausa)\n"
          ]
        }
      ],
      "source": [
        "login(\"hf access token key\")\n",
        "\n",
        "# List of desired target languages ISO codes(to pair with English Language)\n",
        "target_language = {\"ha\" : \"Hausa\"}\n",
        "\n",
        "source_language = \"en\" # Fixed source language\n",
        "\n",
        "desired_pairs = [f\"{source_language}-{tgt}\" for tgt in target_language]\n",
        "\n",
        "# Fetch all configurations from Opus100\n",
        "available_configs = get_dataset_config_names(\"opus100\")\n",
        "\n",
        "# Filter those that exist in Opus100\n",
        "present_pairs = [pair for pair in desired_pairs if pair in available_configs]\n",
        "missing_pairs = [pair for pair in desired_pairs if pair not in available_configs]\n",
        "\n",
        "# Print results\n",
        "print(\" The English-Hausa language pair is present in Opus100 dataset:\")\n",
        "for pair in present_pairs:\n",
        "    print(f\" - {pair} ({target_language[pair.split('-')[1]]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMTkEkrisEPK"
      },
      "source": [
        "#### **Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLOJlAoOsEM2",
        "outputId": "61a4fd15-6424-4404-de22-23a6ba2662ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English-Hausa language pair downloaded!\n"
          ]
        }
      ],
      "source": [
        "# English-Yoruba\n",
        "selected_language = [\"ha\"]\n",
        "\n",
        "if \"ha\" in selected_language:\n",
        "    try:\n",
        "        opus_en_ha = load_dataset(\"opus100\", \"en-ha\")\n",
        "        print(\"English-Hausa language pair downloaded!\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to download English-Hausa:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBRXB-QEsECn"
      },
      "source": [
        "## **Qualitative and Quantitative Examination of Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sP_fn47_KhD"
      },
      "source": [
        "The following is a check-list to examine each dataset:\n",
        "\n",
        "1. Splits\n",
        "2. Features/columns\n",
        "3. Format\n",
        "4. Missing row\n",
        "5. Check internal usaga of language pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw8QvOLuCwW4"
      },
      "source": [
        "### **Splits Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpROvuCBAZX1"
      },
      "outputs": [],
      "source": [
        "# Check splits\n",
        "def print_split_info(name, dataset):\n",
        "    print(f\"\\nüìä Dataset: {name}\")\n",
        "\n",
        "    if isinstance(dataset, DatasetDict):\n",
        "        for split_name, split in dataset.items():\n",
        "            print(f\"  ‚û§ Split: {split_name} | Rows: {split.num_rows}\")\n",
        "    elif isinstance(dataset, Dataset):\n",
        "        print(f\"  ‚û§ Single split | Rows: {dataset.num_rows}\")\n",
        "    else:\n",
        "        print(\"‚ùå Unrecognized dataset type.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXGZYUTUAfLg",
        "outputId": "434d7fc3-c96f-4c7e-9183-65a375f55db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Dataset: Opus100: English-Hausa Language Pair\n",
            "  ‚û§ Split: test | Rows: 2000\n",
            "  ‚û§ Split: train | Rows: 97983\n",
            "  ‚û§ Split: validation | Rows: 2000\n"
          ]
        }
      ],
      "source": [
        "print_split_info(\"Opus100: English-Hausa Language Pair\", opus_en_ha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339LSrt-CzqX"
      },
      "source": [
        "### **Features/Columns/Schema Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8JngOza_9R1"
      },
      "outputs": [],
      "source": [
        "# Check schema/column names using just the train split\n",
        "def print_dataset_features(datasets_with_names):\n",
        "    \"\"\"\n",
        "    Prints the .features of multiple Hugging Face datasets with names.\n",
        "\n",
        "    Args:\n",
        "        datasets_with_names (list of tuples): List of (dataset, name) pairs.\n",
        "    \"\"\"\n",
        "    for dataset, name in datasets_with_names:\n",
        "        print(f\"\\nüìò Features for: {name}\")\n",
        "        print(dataset.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2PbhGxesD-O",
        "outputId": "68420899-8706-44a6-fc81-b913852017d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìò Features for: Opus EN-HA\n",
            "{'translation': Translation(languages=['en', 'ha'], id=None)}\n"
          ]
        }
      ],
      "source": [
        "print_dataset_features([\n",
        "    (opus_en_ha['train'], \"Opus EN-HA\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg6fv3ljsD7-"
      },
      "source": [
        "### **NLLB-Format-Compatibility Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aBzZIbQosD6b",
        "outputId": "fa410bcc-cd3b-4dc9-f13a-b8ced88001f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n{\\n  \"translation\": {\\n    \"source language\": \"source sentence\",\\n    \"target language\": \"target sentence\"\\n  }\\n}\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The NLLB model expects the following format:\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  \"translation\": {\n",
        "    \"source language\": \"source sentence\",\n",
        "    \"target language\": \"target sentence\"\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-cztQtHQgo"
      },
      "source": [
        "#### **Recast Dataset to NLLB Format**\n",
        "\n",
        "To recast your dataset to the NLLB format, you need to ensure two key things:\n",
        "\n",
        "1. The translation feature uses a tuple of language codes, not a list.\n",
        "\n",
        "2. This format aligns with what the NLLB tokenizer expects, especially for multilingual training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6qrnephHQFB"
      },
      "outputs": [],
      "source": [
        "# Define the correct NLLB-style features with a tuple\n",
        "translation_features = Features({\n",
        "    \"translation\": Translation(languages=(\"en\", \"ha\"))  # <- Use tuple\n",
        "})\n",
        "\n",
        "# Recast all splits to match NLLB format\n",
        "opus_en_ha = DatasetDict({\n",
        "    split: ds.cast(translation_features)\n",
        "    for split, ds in opus_en_ha.items()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuzSC57yHQBr",
        "outputId": "6e2ac85a-1677-42f3-ef61-7fc4de322383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'translation': Translation(languages=('en', 'ha'), id=None)}\n"
          ]
        }
      ],
      "source": [
        "# Verify format\n",
        "print(opus_en_ha[\"train\"].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNijpKsUsDyF"
      },
      "source": [
        "### **Check Missing Row**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRfQs3GgsDui"
      },
      "outputs": [],
      "source": [
        "def check_missing_rows_all_splits(dataset_dict, name, src_lang=None, tgt_lang=None):\n",
        "    \"\"\"\n",
        "    Checks for missing or invalid rows across all splits in a DatasetDict.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict (DatasetDict): The dataset with multiple splits.\n",
        "        name (str): Dataset name for reporting.\n",
        "        src_lang (str): Source language key (e.g., 'en').\n",
        "        tgt_lang (str): Target language key (e.g., 'ha').\n",
        "    \"\"\"\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        total = len(split_dataset)\n",
        "        missing = 0\n",
        "\n",
        "        for row in split_dataset:\n",
        "            try:\n",
        "                if \"translation\" in row:\n",
        "                    trans = row[\"translation\"]\n",
        "                    src = trans.get(src_lang, \"\").strip() if src_lang else \"\"\n",
        "                    tgt = trans.get(tgt_lang, \"\").strip() if tgt_lang else \"\"\n",
        "                else:\n",
        "                    src = row.get(src_lang, \"\").strip()\n",
        "                    tgt = row.get(tgt_lang, \"\").strip()\n",
        "\n",
        "                if not src or not tgt or len(src) <= 1 or len(tgt) <= 1:\n",
        "                    missing += 1\n",
        "            except Exception:\n",
        "                missing += 1\n",
        "\n",
        "        print(f\"üîç {name} ({split_name}): {missing} missing / {total} total rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn8hyABDsDsc",
        "outputId": "e9812363-3eb3-451f-f705-86ef4741f6a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Opus EN-HA (test): 0 missing / 2000 total rows\n",
            "üîç Opus EN-HA (train): 12 missing / 97983 total rows\n",
            "üîç Opus EN-HA (validation): 0 missing / 2000 total rows\n"
          ]
        }
      ],
      "source": [
        "# Run checks\n",
        "check_missing_rows_all_splits(opus_en_ha, \"Opus EN-HA\", src_lang=\"en\", tgt_lang=\"ha\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ignb7_4wsDJu"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M_FHykxsDHg"
      },
      "source": [
        "Before we clean, here are some highlight of observations in the examination stage:\n",
        "\n",
        "*  Missing rows\n",
        "  * Opus EN-HA (train) has 12 missing rows  \n",
        "  We eliminate missing rows in this stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y__pI6DfsC2s"
      },
      "source": [
        "### **Clear Missing Rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0DyAamasC0J"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "def clean_missing_rows(dataset_dict, src_lang=None, tgt_lang=None):\n",
        "    \"\"\"\n",
        "    Removes missing/invalid rows across all splits in a DatasetDict.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict (DatasetDict): The dataset with splits (e.g. train, test, validation).\n",
        "        src_lang (str): Source language key (e.g., 'en').\n",
        "        tgt_lang (str): Target language key (e.g., 'ha').\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict: Cleaned dataset with bad rows removed.\n",
        "    \"\"\"\n",
        "    cleaned_splits = {}\n",
        "\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        def is_valid(row):\n",
        "            try:\n",
        "                if \"translation\" in row:\n",
        "                    src = row[\"translation\"].get(src_lang, \"\").strip()\n",
        "                    tgt = row[\"translation\"].get(tgt_lang, \"\").strip()\n",
        "                else:\n",
        "                    src = row.get(src_lang, \"\").strip()\n",
        "                    tgt = row.get(tgt_lang, \"\").strip()\n",
        "                return bool(src and tgt and len(src) > 1 and len(tgt) > 1)\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "        print(f\"üßπ Cleaning split: {split_name}...\")\n",
        "        cleaned_split = split_dataset.filter(is_valid)\n",
        "        cleaned_splits[split_name] = cleaned_split\n",
        "        print(f\"‚úÖ {len(cleaned_split)} rows retained from {len(split_dataset)}\")\n",
        "\n",
        "    return DatasetDict(cleaned_splits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSbD4cVFsCxX",
        "outputId": "205d3de8-a794-4026-8095-850e365ca228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Cleaning split: test...\n",
            "‚úÖ 2000 rows retained from 2000\n",
            "üßπ Cleaning split: train...\n",
            "‚úÖ 97971 rows retained from 97983\n",
            "üßπ Cleaning split: validation...\n",
            "‚úÖ 2000 rows retained from 2000\n"
          ]
        }
      ],
      "source": [
        "opus_en_ha_clean = clean_missing_rows(opus_en_ha, src_lang=\"en\", tgt_lang=\"ha\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghLvHPs7IavA"
      },
      "source": [
        "### **Standardize Feature Schema: Confirmation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFJaXeoxIk28"
      },
      "outputs": [],
      "source": [
        "# Define the correct NLLB-style features with a tuple\n",
        "translation_features = Features({\n",
        "    \"translation\": Translation(languages=(\"en\", \"ha\"))  # <- Use tuple\n",
        "})\n",
        "\n",
        "# Recast all splits to match NLLB format\n",
        "opus_en_ha_clean = DatasetDict({\n",
        "    split: ds.cast(translation_features)\n",
        "    for split, ds in opus_en_ha.items()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfjekM2LIvG5",
        "outputId": "2a7a5e8d-a89c-4b08-cfef-3d86bbbcc5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'translation': Translation(languages=('en', 'ha'), id=None)}\n"
          ]
        }
      ],
      "source": [
        "print(opus_en_ha_clean[\"train\"].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxThvG09sCPX"
      },
      "source": [
        "Everything looks fine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXznBXbhsCM1"
      },
      "source": [
        "## **Save Dataset to Disc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "50ec2cd4144b4e21953e4b17a9bd9d33",
            "f9b1795ce600473daf6b459d03b38f4d",
            "1a61c10d8deb4a549cfc70f8571b949d",
            "043ea61f34d94a9582ea7128c9645147",
            "8767e03f7c03407da373a50c69722b1e",
            "65e56d28fa934bbd82b3879ddd75dcdc",
            "979fe6b2d82f436e8683c00068e13057",
            "c0e8863589034d39814fc633322d3284",
            "ec8bc407a138418491e88bfe26c112e7",
            "880b1dce33494640bdf3d770ec197c37",
            "a2876fdb0b6c4df7a638c332bb894f17",
            "826383c1ce01488390f7dd56e68d7c5d",
            "aa4a38f8f78541bba1048d4b0acb5f83",
            "a8c7151169454b64bab65606db08ff4c",
            "a549da36809c40a2bda9a971da7398c7",
            "21cb68e011bc422899b2e7bf8eb510b6",
            "01b95279f8614e89825d726366af96ca",
            "3bffb1627f8e4c9483f7379c99cd7023",
            "7f0817eed3814437aaef9dc47aa7a68f",
            "1936ae10c2be45cf97994a7bb2f8daa4",
            "924eb204b9984be489d99d42c301970e",
            "2bb27dfdc1784958b5b5faf283d90e28",
            "1355b5553ee6413eb4fbf38600940040",
            "a27836a7da984829b38faf2fc85fa339",
            "45ac9ea656db44ecb7b1f9d7b9fd8243",
            "1628a8c8e0c647889b58a722fb9a4a57",
            "03205b138c5f408396512844482de150",
            "7b2e191d065c410abfdcc07e1efb3d96",
            "b7509952c6d549b5b964229fbc95a03d",
            "d9182fb58b4343c29f71dbb7b5abe57d",
            "10d2597737a9498aa15ad378cc4c5897",
            "9e12ec8f450a4b85981a7bce6bbae6fc",
            "9135de409b6c4dd5941baa0c4a0f6d41"
          ]
        },
        "id": "CrkWvHwksCKn",
        "outputId": "e72303a1-f911-427f-e1e5-8adefe826bbc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50ec2cd4144b4e21953e4b17a9bd9d33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "826383c1ce01488390f7dd56e68d7c5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/97983 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1355b5553ee6413eb4fbf38600940040",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset saved to: /content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ha_LaTn/CleanedHausaDataset\n"
          ]
        }
      ],
      "source": [
        "save_path = f\"{save_dir}/CleanedHausaDataset\"\n",
        "opus_en_ha_clean.save_to_disk(save_path)\n",
        "print(f\"‚úÖ Dataset saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6M7yEkAsCGx"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8jUXsqsp8wB"
      },
      "source": [
        "### **Load Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXEK4cgUp8tN"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "tokenizer.padding_side = \"right\" #  proper for attention mask alignment and decoder positioning for encoder-decoder model like NLLB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emM8wGTBp8p6"
      },
      "source": [
        "### **Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1vp_iykp8K9"
      },
      "outputs": [],
      "source": [
        "opus_dataset = opus_en_ha_clean\n",
        "\n",
        "# Isolate data splits\n",
        "train_dataset = opus_dataset[\"train\"]\n",
        "val_dataset = opus_dataset[\"validation\"]\n",
        "test_dataset = opus_dataset[\"test\"]\n",
        "\n",
        "\n",
        "# Set the target language for NLLB tokenizer globally\n",
        "tokenizer.src_lang = \"eng_Latn\"\n",
        "tokenizer.tgt_lang = \"hau_Latn\"\n",
        "\n",
        "def preprocess(example):\n",
        "    source = example.get(\"translation\", {}).get(\"en\", None)\n",
        "    target = example.get(\"translation\", {}).get(\"ha\", None)\n",
        "\n",
        "    if not source or not target:\n",
        "        return {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "            }\n",
        "\n",
        "    # Add source language prefix for NLLB-style\n",
        "    input_text = f\">>hau_Latn<< {source}\"\n",
        "\n",
        "    # Tokenize source and target using set lang codes\n",
        "    model_inputs = tokenizer(\n",
        "        input_text,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    # Target tokenization works correctly if tgt_lang is already set\n",
        "    target_inputs = tokenizer(\n",
        "        target,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = target_inputs[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlpgDiFBXhcp"
      },
      "outputs": [],
      "source": [
        "# Map Preprocessing on all splits\n",
        "\n",
        "train_tokenized = train_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,  # Optional: use multiple processes\n",
        "    desc=\"Tokenizing train set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)\n",
        "\n",
        "val_tokenized = val_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,\n",
        "    desc=\"Tokenizing val set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)\n",
        "\n",
        "test_tokenized = test_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,\n",
        "    desc=\"Tokenizing test set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MSZL1Txp7_R",
        "outputId": "ce501479-f33d-497f-8203-fe91a7b91918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [256047, 20545, 256066, 57642, 104, 248199, 196581, 1184, 17713, 16701, 452, 7177, 225606, 7131, 37517, 1482, 5884, 12516, 110432, 1288, 44154, 540, 109182, 3559, 452, 6158, 2820, 2986, 62098, 7197, 2790, 248079, 21665, 36972, 924, 540, 21295, 924, 248079, 19553, 6370, 11166, 3022, 349, 13365, 8852, 6370, 4062, 280, 254, 1008, 68, 4659, 248079, 2975, 2040, 11511, 41, 9655, 174, 47703, 248075, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [256047, 1004, 1023, 200, 33813, 36484, 6902, 35766, 6617, 953, 243145, 240, 327, 863, 21271, 170, 2548, 15, 2286, 327, 197796, 6318, 12030, 170, 70966, 919, 4781, 248332, 327, 248079, 9, 388, 165135, 170, 2872, 23478, 248079, 6318, 362, 19930, 5531, 9275, 40, 1344, 160, 2535, 248079, 33813, 276, 150097, 9, 27308, 248079, 2286, 33813, 276, 513, 29017, 185, 248075, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "# Check a sample from tokenized data to confirm tokenization\n",
        "print(train_tokenized[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LcPpKA-Op78N",
        "outputId": "0e6a03d9-ab30-46db-ed86-9a551ee62978"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n{\\n  'input_ids': [...],\\n  'attention_mask': [...],\\n  'labels': [...]\\n}\\n\\n\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The strcture above is interpreted as:\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  'input_ids': [...],\n",
        "  'attention_mask': [...],\n",
        "  'labels': [...]\n",
        "}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSS3vb437RVL",
        "outputId": "a9a00912-3ca2-484e-9110-5574af5881dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 97983\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM2dqks9p7zG"
      },
      "source": [
        "## **Fine-Tune NLLB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHFNAuXBp7wZ"
      },
      "source": [
        "### **Configure BitsAndBytes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRktXaopp7tQ"
      },
      "outputs": [],
      "source": [
        "# BitsAndBytes parameters\n",
        "################################################################################\n",
        "use_4bit = True # 4-bit precision on base model loading\n",
        "bnb_4bit_compute_dtype = torch.float16 # compute datatype for 4-bit base model\n",
        "bnb_4bit_quant_type = \"nf4\" # quantization type\n",
        "use_nested_quant = False # activate nested quantization for 4-bit base models (double quantization)\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je9N6UmhsCAA"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint,\n",
        "                                              device_map=\"auto\",\n",
        "                                              low_cpu_mem_usage=True,  # Explicitly set to avoid the warning\n",
        "                                              quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S4n34njN6AF"
      },
      "source": [
        "### **Test Model with Zero Shot Inferencing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhElREp_N5f8",
        "outputId": "89be70ab-bb2b-463a-8d44-258d7249d76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî§ English: The weather today is very pleasant.\n",
            "üåç Hausa: Yanayin yau yana da kyau sosai.\n",
            "CPU times: user 770 ms, sys: 172 ms, total: 942 ms\n",
            "Wall time: 1.58 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# üåç Define source & target languages (ISO 639-3 codes)\n",
        "src_lang = \"eng_Latn\"\n",
        "tgt_lang = \"hau_Latn\"\n",
        "\n",
        "# ‚úèÔ∏è Example input sentence in English\n",
        "input_sentence = \"The weather today is very pleasant.\"\n",
        "\n",
        "# üî° Tokenize with language codes\n",
        "inputs = tokenizer(\n",
        "    input_sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# ‚ú® Set language tokens\n",
        "inputs[\"forced_bos_token_id\"] = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "tokenizer.src_lang = src_lang\n",
        "\n",
        "# üîÅ Run inference\n",
        "with torch.no_grad():\n",
        "    output_tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "# üó£Ô∏è Decode result\n",
        "translated_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
        "print(f\"üî§ English: {input_sentence}\")\n",
        "print(f\"üåç Hausa: {translated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_ByOm0WsB95"
      },
      "source": [
        "### **Setup Model with LoRA (PEFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFqF8EjfsB7k"
      },
      "outputs": [],
      "source": [
        "# Set up LoRA config with target_modules\n",
        "lora_config = LoraConfig(\n",
        "    r = 8,  # Rank of the decomposition\n",
        "    lora_alpha = 32,  # Scaling factor for LoRA updates\n",
        "    lora_dropout = 0.05,  # Dropout rate for LoRA\n",
        "    task_type = TaskType.SEQ_2_SEQ_LM,  # Sequence-to-sequence task\n",
        "    bias = 'none',\n",
        "    target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers (query, value, key, output)\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters to the model\n",
        "model_with_lora = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wrLctjFsB5V",
        "outputId": "f0ddc48a-7edf-4f38-b8a1-cee29c07dbe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 616,843,264 || trainable%: 0.2869\n"
          ]
        }
      ],
      "source": [
        "# Check trainable parameters\n",
        "model_with_lora.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYBKR9rJsB2B"
      },
      "source": [
        "### **Prepare DataCollator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7fgOW3EsBzf"
      },
      "outputs": [],
      "source": [
        "# Instantiate the data collator for sequence-to-sequence tasks\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer,\n",
        "                                       model = model_with_lora,\n",
        "                                       padding = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Ps9qrwsBx0"
      },
      "source": [
        "### **Define Seq2SeqTrainingArguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEdtHMzusBvZ"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    eval_strategy = \"epoch\",  # Evaluate after every epoch\n",
        "    logging_dir = f\"{save_dir}/logs\",  # Directory for storing logs\n",
        "    logging_strategy = \"steps\",  # Log every N steps\n",
        "    logging_steps = 25,  # Log every 25 steps\n",
        "    save_strategy = \"epoch\",  # Save model after every epoch\n",
        "    save_total_limit = 3,  # Keep only the latest 3 checkpoints\n",
        "    per_device_train_batch_size = 4,  # Batch size per device for training\n",
        "    per_device_eval_batch_size = 4,  # Batch size per device for evaluation\n",
        "    gradient_accumulation_steps = 2,  # Accumulate gradients for 2 steps before updating weights\n",
        "    num_train_epochs = 1,  # Total number of epochs\n",
        "    predict_with_generate = True,  # Predict with generate\n",
        "    weight_decay = 0.01,  # Weight decay\n",
        "    lr_scheduler_type = \"linear\",  # Linear learning rate scheduler\n",
        "    optim = \"paged_adamw_32bit\",  # Optimizer to use\n",
        "    learning_rate = 2e-5,  # Initial learning rate\n",
        "    eval_steps = 500, # run validation every 500 steps\n",
        "    fp16 = True,  # Use mixed precision training (not recommended for faster training on GPUs, especially A100 GPUs)\n",
        "    load_best_model_at_end = True,  # Load the best model at the end based on evaluation metric\n",
        "    metric_for_best_model = \"eval_loss\",  # Metric to monitor for the best model (e.g., BLEU score for translation)\n",
        "    greater_is_better = False,  # Higher BLEU metric scores are better\n",
        "    report_to = \"none\",  # Use TensorBoard for logging\n",
        "    disable_tqdm = False,  # Enable or disable tqdm (progress bar)\n",
        "    save_steps = 500,  # Save model checkpoints every 500 steps\n",
        "    label_names = [\"labels\"],  # Name of the label column in the dataset\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IOQD149N4S"
      },
      "source": [
        "### **Compute Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzYXaeC49N0m"
      },
      "outputs": [],
      "source": [
        "# metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# def compute_metrics(eval_preds):\n",
        "#     preds, labels = eval_preds\n",
        "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "#     result = metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "#     return {\"bleu\": result[\"score\"]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC7Kcd3e9NyL"
      },
      "source": [
        "### **Training Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oR-8S_C9Nte"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_with_lora,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized.select(range(min(len(val_tokenized), 2000))),\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics  # compute BLEU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW_SqH4u9Nru"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "34MRpJpr9NhC",
        "outputId": "70e16a88-c946-443c-bbda-fb984c89e0b5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training starts at 1747760788.2398384\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4494' max='12248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4494/12248 35:38 < 1:01:31, 2.10 it/s, Epoch 0.37/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12248' max='12248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12248/12248 1:36:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>6.172800</td>\n",
              "      <td>6.112186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ends at 1747766563.8209355\n",
            "Training time: 1h 36m 15s\n"
          ]
        }
      ],
      "source": [
        "# Compute total training time\n",
        "start_time = time.time()\n",
        "print(f\"Training starts at {start_time}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training ends at {end_time}\")\n",
        "\n",
        "total_seconds = end_time - start_time\n",
        "hours = int(total_seconds // 3600)\n",
        "minutes = int((total_seconds % 3600) // 60)\n",
        "seconds = int(total_seconds % 60)\n",
        "\n",
        "print(f\"Training time: {hours}h {minutes}m {seconds}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60KJ7ufX9NfX"
      },
      "source": [
        "## **Save Model and Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-D2ibw39Ndx",
        "outputId": "fcef9d43-7c19-4709-d638-68db87a8259b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ha_LaTn/En-Ha_FT_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ha_LaTn/En-Ha_FT_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ha_LaTn/En-Ha_FT_model/sentencepiece.bpe.model',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ha_LaTn/En-Ha_FT_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ha_LaTn/En-Ha_FT_model/tokenizer.json')"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(f\"{save_dir}/En-Ha_FT_model\")\n",
        "tokenizer.save_pretrained(f\"{save_dir}/En-Ha_FT_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrz9Z-8SHlUY"
      },
      "source": [
        "## **Push to Hugging Face**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2x6BfgWHlQE",
        "outputId": "2c805ded-65d6-43e2-84e4-7df04668a6b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# !pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp8l1bHIHmMW",
        "outputId": "f0e67196-0738-49b1-e389-7a5e6c9be8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "The token `DibiaCorp` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `DibiaCorp`\n"
          ]
        }
      ],
      "source": [
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "04f2984b814b4983a1dd40e05679b4c7",
            "968029636e944bd395eebdf2b42d5eac",
            "eba4cb0fe10d4f57a3c623f6811b39a4",
            "db33575be0f246059d514be057cd3081",
            "4618570f4c524529addac7d7248750f2",
            "645266ed992541cb91b48e73a87cf262",
            "36f65bae24684daab7db79b0e09ec26a",
            "878af3345e6845b4b9a2a78b3c246a57",
            "2b2cedf65d994db38ef4373d64fde527",
            "fe1c68469e4049efa6ed946ddcc1ee29",
            "3d13c01a6eab44de847c61b439f7de25",
            "127e2e98f8f546eb91536a908d5aa5a6",
            "7ad6a1690c2240f082b660efdbce1d4c",
            "6c6502245ec34c04b1ddbcb2a38a8465",
            "c89f5f23d1994e4c991792667f47a978",
            "c6c4bc3a418142a184c3b89ee96f666e",
            "c8ff40a288cf4294be5fdf9e235316cf",
            "57fad8792ce5446b831f73218d9278b4",
            "6d1f36ea6c4f4bd0a4f727405e5ed46e",
            "b76797c9328544f787413d5520cbeb76",
            "d409f678b2c64d3f946c0cde087273d7",
            "2246024f37fc47c0861df78d52669ed3",
            "7a50bf234c7f47c3990c2d1f84a18b79",
            "ac4aa3f76034417e8632dd2d78b863ed",
            "5377f01be8d84caa9849945fdd3644a0",
            "22729fd5a33d46039abe2649d0c5d387",
            "4fa2a2e34040482c81d044d11079e307",
            "ffa12e6bdb8c459498aff8be6c285c08",
            "e21cd0ce8237448784d8595ab8245967",
            "9916502b37b14311b7271a51513b62be",
            "32e72bc820a641ab889501549a6a9277",
            "9d63b12cadb041faa17f6053a47b9bb0",
            "cbb58215dda649799043d62f47e40b4e",
            "d9d945a4f36545cc863d3155b79fd028",
            "2f243beb9bca4d0c9f79ddf729ccc48e",
            "0e61bf9433f94a0eb7e1649d7c321f88",
            "e786b359e9ac45b7a283546acd637f70",
            "e17400c4fbfa4b709ae8e60878dd4987",
            "99324e8c3a144a7d845b40d072347878",
            "6ffb4110e63f44a4bea3574348a8569f",
            "8a63fc5863d544758d654494b734c934",
            "7af4dd96172347529ddc86dbf72998f0",
            "55e1dd25f37b44868aa09b172deeac80",
            "63c9046901314cfea762cffe19a1809e",
            "2d12666ce048465ea71ed649f75c52e5",
            "3c7a91ae8b4f493ea675fe7621901df9",
            "e3a7f43458c54509969f95acfc9e9e3a",
            "4b985742df6b4a518c1f9ccc4ec6ad83",
            "9da4439f3cb84cfeade002587fd28ee6",
            "1a47048a31194c469ec6c68cd3d608c7",
            "1b30796f94ea4b1092204c0f3a221c7a",
            "5f300b186a424efeada78374111a21da",
            "517255bbd5fd49ff8456cde4ac426e66",
            "c6cd9fde4ad14e71b2cdf647f3af5419",
            "8cebd84919ed40cdb45db034117614d6"
          ]
        },
        "id": "A1hgb1e4HlNj",
        "outputId": "6d1cc817-db2b-4b19-ebbd-e98704ebfb48"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04f2984b814b4983a1dd40e05679b4c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/7.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "127e2e98f8f546eb91536a908d5aa5a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a50bf234c7f47c3990c2d1f84a18b79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9d945a4f36545cc863d3155b79fd028",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d12666ce048465ea71ed649f75c52e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/32.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/drakensberg85/English-Hausa_NLLB_FT_model/commit/429f24fdfb0d7f0e082346d5b33741f5c47d159e', commit_message='Upload tokenizer', commit_description='', oid='429f24fdfb0d7f0e082346d5b33741f5c47d159e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/drakensberg85/English-Hausa_NLLB_FT_model', endpoint='https://huggingface.co', repo_type='model', repo_id='drakensberg85/English-Hausa_NLLB_FT_model'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from huggingface_hub import HfApi, HfFolder\n",
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# # Define repo details\n",
        "# repo_name = \"drakensberg85/English-Hausa_NLLB_FT_model\"\n",
        "# model_path = f\"{save_dir}/En-Ha_FT_model\"\n",
        "\n",
        "# # Upload using transformers\n",
        "# AutoModelForSeq2SeqLM.from_pretrained(model_path).push_to_hub(repo_name)\n",
        "# AutoTokenizer.from_pretrained(model_path).push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI3p6I-d9NZH"
      },
      "source": [
        "## **TensorBoard Logging and Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcKoftke9NXZ"
      },
      "outputs": [],
      "source": [
        "# writer = SummaryWriter(f\"{save_dir}/logs\")\n",
        "# print(\"Training complete. View metrics using TensorBoard:\")\n",
        "# print(f\"Run this in Colab terminal: tensorboard --logdir={save_dir}/logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZQqrrLb9NTd"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir \"/content/drive/MyDrive/Colab Notebooks/NLLB_600M/En-Ha_LaTn/logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igDlKhBK9NQb"
      },
      "source": [
        "## **Inference Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdRXyhpQw_-i",
        "outputId": "c2e83d0b-d694-40a7-8bae-7c0b6d70a54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: A frog on a lily, so still and so sly,Leaps through the hush as the dragonflies fly, Night hums a tune 'neath the moonlit sky.\n",
            "Hausa: Kurciya a kan wata lily, mai sh√£maki, mai sh√£maki, tana tsallewa a cikin shiru kamar kurciya, kuma dare yana yin wa∆ôa a ∆ôar∆ôashin wata mai haske.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Use the correct model path\n",
        "model_path = f\"{save_dir}/En-Ha_FT_model\"\n",
        "\n",
        "# Load tokenizer and model from local files\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# Send model to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Example sentence for translation\n",
        "english_sentence = \"A frog on a lily, so still and so sly,Leaps through the hush as the dragonflies fly, Night hums a tune 'neath the moonlit sky.\"\n",
        "source_sentence = f\">>hau_Latn<< {english_sentence}\"\n",
        "\n",
        "# Tokenize input and move to device\n",
        "inputs = tokenizer(source_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate translation\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)\n",
        "\n",
        "# Decode and print translation\n",
        "hausa_translation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"English: {english_sentence}\")\n",
        "print(f\"Hausa: {hausa_translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spj8CD8Uw_7b"
      },
      "source": [
        "## **Incorporating Chainlit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBI_xyl1w_4g",
        "outputId": "a22e3872-553d-42b8-cdc0-2dfea332dd26"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import chainlit as cl\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_path = f\"{save_dir}/En-Ha_FT_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    await cl.Message(content=\"üëã Welcome! Type something in English and I'll translate it to Hausa!\").send()\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    input_text = f\">>hau_Latn<< {message.content}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Create an empty Chainlit message to stream into\n",
        "    response = cl.Message(content=\"\")\n",
        "    await response.send()\n",
        "\n",
        "    # Generate tokens step-by-step\n",
        "    output_tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=1,  # Beam search disables streaming behavior\n",
        "        do_sample=False,\n",
        "        output_scores=False,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "    # Stream tokens (you can simulate streaming with a short delay per chunk if needed)\n",
        "    output_text = tokenizer.decode(output_tokens.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Simulate streaming (token-by-token)\n",
        "    for token in output_text.split():\n",
        "        response.content += token + \" \"\n",
        "        await response.update()\n",
        "\n",
        "    # Final update\n",
        "    await response.update()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mycaZysfw_o4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}