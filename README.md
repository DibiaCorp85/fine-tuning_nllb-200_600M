# ğŸ§  Fine-Tuning NLLB for Language Translation

This project fine-tunes Meta AIâ€™s NLLB (No Language Left Behind) model to translate from **English** to **selected language (LaTn)**, especially low-resource languages. The training pipeline leverages Hugging Faceâ€™s ecosystem to tokenize data, fine-tune multilingual transformers, and evaluate translation quality using standard metrics.

---

## ğŸ“Œ Project Highlights

- ğŸ”¤ Language Pair: `English â†’ ....`
- ğŸ§° Model Base: [facebook/nllb-200-distilled-1.3B](https://huggingface.co/facebook/nllb-200-distilled-600M)
- âš™ï¸ Tools: Hugging Face Transformers, Datasets, Evaluate, and Colab (or local GPU)
- ğŸ“„ Format: Python code implemented in a Jupyter Notebook

---

## ğŸš€ Example Usage

### âœ… Install Required Packages

```bash
pip install transformers datasets sacrebleu evaluate sentencepiece ...
```

## ğŸ§ª Evaluation
After training, the model is evaluated using:

BLEU Score â€“ Measures n-gram overlap between candidate and reference translations.

## ğŸ™ Acknowledgements
- Meta AI â€“ for releasing the NLLB-200 model
- Hugging Face â€“ for transformers, datasets, and evaluation libraries
- JW300 / MENYO-20k / ROGENDO â€“ multilingual parallel corpora for low-resource African languages
- Google Colab â€“ free GPU access for prototyping and training


## Take-Away
Efficient translation models capable of an appreciable degree of accurate English-to-African-language translation. These notebooks show the implementation steps for a Seq2Seq fine-tuning process.
