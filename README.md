# ğŸ§  Fine-Tuning NLLB for Englishâ€“Wolof Translation (Latin Script)

This project fine-tunes Meta AIâ€™s NLLB (No Language Left Behind) model to translate from **English** to **Wolof (LaTn)**, a low-resource language using the Latin script. The training pipeline leverages Hugging Faceâ€™s ecosystem to tokenize data, fine-tune multilingual transformers, and evaluate translation quality using standard metrics.

---

## ğŸ“Œ Project Highlights

- ğŸ”¤ Language Pair: `English â†’ Wolof (LaTn)`
- ğŸ§° Model Base: [facebook/nllb-200-distilled-1.3B](https://huggingface.co/facebook/nllb-200-distilled-1.3B)
- âš™ï¸ Tools: Hugging Face Transformers, Datasets, Evaluate, and Colab (or local GPU)
- ğŸ“„ Format: Python code implemented in a Jupyter Notebook

---

## ğŸš€ Example Usage

### âœ… Install Required Packages

```bash
pip install transformers datasets sacrebleu evaluate sentencepiece ...
```

## ğŸ§ª Evaluation
After training, the model is evaluated using:

BLEU Score â€“ Measures n-gram overlap between candidate and reference translations.

## ğŸ™ Acknowledgements
- Meta AI â€“ for releasing the NLLB-200 model
- Hugging Face â€“ for transformers, datasets, and evaluate libraries
- JW300 / MENYO-20k / ROGENDO â€“ multilingual parallel corpora for low-resource African languages
- Google Colab â€“ free GPU access for prototyping and training


## Warning
Translations in some way are not very accurate. However, this note shows the implementation steps for a Seq2Seq fine-tuning process.
