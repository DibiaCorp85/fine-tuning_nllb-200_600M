{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DibiaCorp85/fine-tuning_nllb-200_600M/blob/main/_Fine_Tuning_En_Sw_LaTn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46HOKOHDsASy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMTxEDN6sEjk"
      },
      "source": [
        "## **Install Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrAh2JE3sEf8",
        "outputId": "2c86e6f8-9486-4b4f-a9b6-2979a52ff771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.6/800.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet chainlit pyngrok datasets transformers huggingface_hub[hf_xet] evaluate accelerate peft sacrebleu rouge_score bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP3dw5L8D204",
        "outputId": "38337d1c-bcf3-44f9-e166-e3d67b1d415c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade fsspec datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y9wq391sEdk"
      },
      "source": [
        "## **Import Core Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7ir1BGVsEa_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from datasets import (load_dataset,\n",
        "                      concatenate_datasets,\n",
        "                      DatasetDict,\n",
        "                      Dataset,\n",
        "                      get_dataset_config_names,\n",
        "                      Features,\n",
        "                      ClassLabel,\n",
        "                      Value,\n",
        "                      Translation\n",
        "                      )\n",
        "\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        ")\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "import getpass\n",
        "from pyngrok import conf, ngrok\n",
        "import subprocess\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB89k0d3sEXI",
        "outputId": "7dc7c2a8-9a97-4802-a10b-6d43b551ff28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to save model and logs\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Sw_LaTn\"\n",
        "os.makedirs(save_dir, exist_ok = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGPT4cP7sEUt"
      },
      "source": [
        "## **Load Datasets**\n",
        "\n",
        "English-\"Language\" datasets are loaded from Hugging Face.\n",
        "\n",
        "For this process, the following language is considered:\n",
        "* Swahili\n",
        "\n",
        "The following are the datasets are used: `rogendo/english-swahili-sentence-pairs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wxoGrp5SpN"
      },
      "source": [
        "### **Swahili Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "183ef36ab39f4046828f72521adae27e",
            "a2344b83ba9e41a9a4e804d27b4ad874",
            "ef79728e4bef4991a0baa062a51addda",
            "653837584b5949cea0a8f1799b33b938",
            "12edc7f209c048afbd78587732b0f831",
            "eb49fae5d52f4d70a5520bcf9e513523",
            "ce797b5fe1cb4babbb069c20f0575ae7",
            "513f0218a4454cfeacf9537acb659f65",
            "bb7116b6ce294d5bb3153bcd29a28238",
            "28ce5110c2974c769a7ee430fee85bd9",
            "663661420f314aa59c471fb1f59783b2",
            "289cec353f154081bc0b1e5d4c9630a6",
            "073f2bd02adb4b3c9e6502fd07a4abcd",
            "9c3c0f8ac423483f9d508e1327be3e40",
            "2111512c4ad945809893f937f7f52db9",
            "d470528ae0004dc8b4aae096fc2be26c",
            "0c13e2732ff148e6a43ba24bad8f4eef",
            "1da542339df04532a98cd6d0715e7607",
            "c5041e4d85ec448db78e9fd960a0a1a3",
            "c63c9bd9a0d64cc89896fe56d043438c",
            "07711fbd74414cff9554eff1bbd6b508",
            "9a4356b3a4d544ba8fd1b3fa5fa39cd1",
            "713dce6857b2430791384c3ebc659280",
            "92843adb5fd3474588d68e1f3cf858d5",
            "f015d4502ae64631ade799ef571456a9",
            "619c83ebb9424ac5b707dcaa0d3ecefe",
            "a9b6b40570144f518eff47f0d74b3eb4",
            "88ed512cc37049e8a1cbcfc570a3ec6f",
            "88e79884f6d544da92756b6fee89c57e",
            "8d7f42778cd94942a85f29d06b5b328a",
            "1fa6a641a2a44dbe8f13097935645849",
            "146df00297e04229ac41f2d3e292a261",
            "e5354d519cd84eecabd25dafffd8a8ad"
          ]
        },
        "id": "yC2sCb91sERW",
        "outputId": "581a95ab-c512-4416-96a1-062268b8b7ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "183ef36ab39f4046828f72521adae27e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "289cec353f154081bc0b1e5d4c9630a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ensw.csv:   0%|          | 0.00/21.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "713dce6857b2430791384c3ebc659280",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English-Swahili language pair downloaded!\n"
          ]
        }
      ],
      "source": [
        "login(\"HF ACCESS TOKEN KEY\")\n",
        "\n",
        "selected_language = [\"sw\"]\n",
        "\n",
        "# English-Swahili\n",
        "if \"sw\" in selected_language:\n",
        "    try:\n",
        "        swahili = load_dataset(\"rogendo/english-swahili-sentence-pairs\",\n",
        "                             trust_remote_code=True)\n",
        "        print(\"English-Swahili language pair downloaded!\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to download English-Swahili:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBRXB-QEsECn"
      },
      "source": [
        "## **Qualitative and Quantitative Examination of Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sP_fn47_KhD"
      },
      "source": [
        "The following is a check-list to examine each dataset:\n",
        "\n",
        "1. Splits\n",
        "2. Features/columns\n",
        "3. Format\n",
        "4. Missing row\n",
        "5. Check internal usaga of language pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw8QvOLuCwW4"
      },
      "source": [
        "### **Splits Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpROvuCBAZX1"
      },
      "outputs": [],
      "source": [
        "# Check splits\n",
        "def print_split_info(name, dataset):\n",
        "    print(f\"\\nğŸ“Š Dataset: {name}\")\n",
        "\n",
        "    if isinstance(dataset, DatasetDict):\n",
        "        for split_name, split in dataset.items():\n",
        "            print(f\"  â¤ Split: {split_name} | Rows: {split.num_rows}\")\n",
        "    elif isinstance(dataset, Dataset):\n",
        "        print(f\"  â¤ Single split | Rows: {dataset.num_rows}\")\n",
        "    else:\n",
        "        print(\"âŒ Unrecognized dataset type.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXGZYUTUAfLg",
        "outputId": "3fc309b6-e959-49c7-a6b3-2e746175cea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š Dataset: swahili: English-Swah+.. Language Pair\n",
            "  â¤ Split: train | Rows: 210471\n"
          ]
        }
      ],
      "source": [
        "print_split_info(\"swahili: English-Swah+.. Language Pair\", swahili)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339LSrt-CzqX"
      },
      "source": [
        "### **Features/Columns/Schema Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8JngOza_9R1"
      },
      "outputs": [],
      "source": [
        "# Check schema/column names using just the train split\n",
        "def print_dataset_features(datasets_with_names):\n",
        "    \"\"\"\n",
        "    Prints the .features of multiple Hugging Face datasets with names.\n",
        "\n",
        "    Args:\n",
        "        datasets_with_names (list of tuples): List of (dataset, name) pairs.\n",
        "    \"\"\"\n",
        "    for dataset, name in datasets_with_names:\n",
        "        print(f\"\\nğŸ“˜ Features for: {name}\")\n",
        "        print(dataset.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2PbhGxesD-O",
        "outputId": "395ce4d3-d579-4ec4-e26e-a8933a8edf37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“˜ Features for: English-swahili\n",
            "{'English sentence': Value(dtype='string', id=None), 'Swahili Translation': Value(dtype='string', id=None)}\n"
          ]
        }
      ],
      "source": [
        "print_dataset_features([\n",
        "    (swahili['train'], \"English-swahili\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg6fv3ljsD7-"
      },
      "source": [
        "### **NLLB-Format-Compatibility Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccuRF8SLgnzH",
        "outputId": "342eb34a-2eda-460b-e948-4148464e50e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['English sentence', 'Swahili Translation'],\n",
            "    num_rows: 210471\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(swahili['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aBzZIbQosD6b",
        "outputId": "a792a501-dd66-4dcd-b662-2f137572fccb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n{\\n  \"translation\": {\\n    \"source language\": \"source sentence\",\\n    \"target language\": \"target sentence\"\\n  }\\n}\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The NLLB model expects the following format:\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  \"translation\": {\n",
        "    \"source language\": \"source sentence\",\n",
        "    \"target language\": \"target sentence\"\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-cztQtHQgo"
      },
      "source": [
        "#### **Recast Dataset to NLLB Format**\n",
        "\n",
        "To recast your dataset to the NLLB format, you need to ensure two key things:\n",
        "\n",
        "1. The translation feature uses a tuple of language codes, not a list.\n",
        "\n",
        "2. This format aligns with what the NLLB tokenizer expects, especially for multilingual training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e50c88c9664d4634b2d53caeb703a2a3",
            "323873f14b2841f28a01c40988aeeae8",
            "048a93c37a504275ae126fe77d619be4",
            "4c2c46b0f91a4767a9e5a8cf1adc7dfc",
            "a0fb82aa89b64f26affaf93201a6867a",
            "bd60e01d4ba8439e8a18f696f3366b5f",
            "4c4870e111c44ae29fe7846c3aa2d991",
            "48fa26ffe5b4421780dbe2472dfe9abb",
            "b603a8ab226144459afd505861c41408",
            "010ba9a2efab4355850cadb4462c28f1",
            "21d05cfe938d4a3cb6dfff36e5eef4b0",
            "2b8c28c7279f474982b2b548fc8e4960",
            "65003d54a8d24c5a85ae2eea26d17966",
            "452e9c4257f249c7ad207343f7a999dd",
            "d53d8d7675cc4e93a86b871a160e6aa7",
            "8df697b1da6f4644a862f5e8cb163d35",
            "4d806b3cb0f24edd9e7490df886dfd68",
            "8d999ed4951342b3814e5b17f8b9079b",
            "fa394585ce3e438db6c9816ba42aa5a7",
            "5ba07e0cf8164adf9b7a130c48aa1c3a",
            "7ee84e9534584eb79f646299624582db",
            "fc215255bdd34abb9fc486b9738a7b9d"
          ]
        },
        "id": "o6qrnephHQFB",
        "outputId": "6f3745fc-f923-446a-fae1-e76747b66808"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e50c88c9664d4634b2d53caeb703a2a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/210471 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b8c28c7279f474982b2b548fc8e4960",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/210471 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Step 1: Define NLLB translation features\n",
        "nllb_features = Features({\n",
        "    \"translation\": Translation(languages=(\"en\", \"sw\"))\n",
        "})\n",
        "\n",
        "# Step 2: Convert field names into a 'translation' dictionary\n",
        "swahili = swahili.map(\n",
        "    lambda x: {\n",
        "        \"translation\": {\n",
        "            \"en\": x[\"English sentence\"],\n",
        "            \"sw\": x[\"Swahili Translation\"]\n",
        "        }\n",
        "    },\n",
        "    remove_columns=[\"English sentence\", \"Swahili Translation\"]\n",
        ")\n",
        "\n",
        "# Step 3: Cast to enforce schema\n",
        "swahili = swahili.cast(nllb_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuzSC57yHQBr",
        "outputId": "89c3e4ec-b7fd-4748-a274-7a68e3a19048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'translation': Translation(languages=('en', 'sw'), id=None)}\n",
            "{'translation': {'en': 'I am', 'sw': 'mimi ni'}}\n"
          ]
        }
      ],
      "source": [
        "# Verify format\n",
        "print(swahili[\"train\"].features)\n",
        "print(swahili[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNijpKsUsDyF"
      },
      "source": [
        "### **Check Missing Row**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRfQs3GgsDui"
      },
      "outputs": [],
      "source": [
        "def check_missing_rows_all_splits(dataset_dict, name, src_lang=None, tgt_lang=None):\n",
        "    \"\"\"\n",
        "    Checks for missing or invalid rows across all splits in a DatasetDict.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict (DatasetDict): The dataset with multiple splits.\n",
        "        name (str): Dataset name for reporting.\n",
        "        src_lang (str): Source language key (e.g., 'en').\n",
        "        tgt_lang (str): Target language key (e.g., 'sw').\n",
        "    \"\"\"\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        total = len(split_dataset)\n",
        "        missing = 0\n",
        "\n",
        "        for row in split_dataset:\n",
        "            try:\n",
        "                if \"translation\" in row:\n",
        "                    trans = row[\"translation\"]\n",
        "                    src = trans.get(src_lang, \"\").strip() if src_lang else \"\"\n",
        "                    tgt = trans.get(tgt_lang, \"\").strip() if tgt_lang else \"\"\n",
        "                else:\n",
        "                    src = row.get(src_lang, \"\").strip()\n",
        "                    tgt = row.get(tgt_lang, \"\").strip()\n",
        "\n",
        "                if not src or not tgt or len(src) <= 1 or len(tgt) <= 1:\n",
        "                    missing += 1\n",
        "            except Exception:\n",
        "                missing += 1\n",
        "\n",
        "        print(f\"ğŸ” {name} ({split_name}): {missing} missing / {total} total rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn8hyABDsDsc",
        "outputId": "4d9306b9-e529-4c2b-da38-766816c79d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” EN-SW (train): 69 missing / 210471 total rows\n"
          ]
        }
      ],
      "source": [
        "# Run checks\n",
        "check_missing_rows_all_splits(swahili, \"EN-SW\", src_lang=\"en\", tgt_lang=\"sw\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ignb7_4wsDJu"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M_FHykxsDHg"
      },
      "source": [
        "Before we clean, here are some highlight of observations in the examination stage:\n",
        "\n",
        "*  Missing rows\n",
        "  * Swahili (train) has 69 missing rows  \n",
        "  We eliminate missing rows in this stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y__pI6DfsC2s"
      },
      "source": [
        "### **Clear Missing Rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0DyAamasC0J"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "def clean_missing_rows(dataset_dict, src_lang=None, tgt_lang=None):\n",
        "    \"\"\"\n",
        "    Removes missing/invalid rows across all splits in a DatasetDict.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict (DatasetDict): The dataset with splits (e.g. train, test, validation).\n",
        "        src_lang (str): Source language key (e.g., 'en').\n",
        "        tgt_lang (str): Target language key (e.g., 'sw').\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict: Cleaned dataset with bad rows removed.\n",
        "    \"\"\"\n",
        "    cleaned_splits = {}\n",
        "\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        def is_valid(row):\n",
        "            try:\n",
        "                if \"translation\" in row:\n",
        "                    src = row[\"translation\"].get(src_lang, \"\").strip()\n",
        "                    tgt = row[\"translation\"].get(tgt_lang, \"\").strip()\n",
        "                else:\n",
        "                    src = row.get(src_lang, \"\").strip()\n",
        "                    tgt = row.get(tgt_lang, \"\").strip()\n",
        "                return bool(src and tgt and len(src) > 1 and len(tgt) > 1)\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "        print(f\"ğŸ§¹ Cleaning split: {split_name}...\")\n",
        "        cleaned_split = split_dataset.filter(is_valid)\n",
        "        cleaned_splits[split_name] = cleaned_split\n",
        "        print(f\"âœ… {len(cleaned_split)} rows retained from {len(split_dataset)}\")\n",
        "\n",
        "    return DatasetDict(cleaned_splits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "d623fe4a84984584af6247fdb08b0193",
            "9c135217e09047909543e96d07803bb1",
            "0cedc5eb8faa4c978de76244e0447a06",
            "4bb2b79a305d47d29b14070c58d9a19f",
            "41f42e5e05554075a05e82fdd293c233",
            "242abc5d92bc4204b8348458bfcafa03",
            "3f14d83466c64cce87ded9f675bad606",
            "9f9fa7403b5a42e18b49129c361c7133",
            "a24be59af80149869ad8ea9d40e1a792",
            "df2ec51706624f25beb6c775937b6e3a",
            "687c438b38ab4f3d8ed518eb0d594594"
          ]
        },
        "id": "KSbD4cVFsCxX",
        "outputId": "b4108b50-fe1d-4348-c6d2-07ad60adf59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ Cleaning split: train...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d623fe4a84984584af6247fdb08b0193",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/210471 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… 210402 rows retained from 210471\n"
          ]
        }
      ],
      "source": [
        "swahili_clean = clean_missing_rows(swahili, src_lang=\"en\", tgt_lang=\"sw\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghLvHPs7IavA"
      },
      "source": [
        "### **Standardize Feature Schema: Confirmation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7c24a57206ce479d93ead3e481269047",
            "b226cdc1440246fdabb341459a94b107",
            "946d89fcc94e4a81b5b475e77961c132",
            "cbb384ba226b4af18c32acdba1e6cc98",
            "660bd096c0bb4215b658580cfe7f3801",
            "9f66ea50601b43df8837167b9e990b04",
            "ce2c664fb78d499ca1e67ba8eadbeadf",
            "24f10de6e1c04b1a81f54dbed79aa1f2",
            "8ce709802b784b14bfe145b4abcb238d",
            "0e3d5047ef91486b869028bef9000b70",
            "e9b5c1c9f8fc48f5880804e8e3b6dae5"
          ]
        },
        "id": "WFJaXeoxIk28",
        "outputId": "b80f2904-5bb2-440e-b392-0c77148fdcea"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c24a57206ce479d93ead3e481269047",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/210471 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the correct NLLB-style features with a tuple\n",
        "translation_features = Features({\n",
        "    \"translation\": Translation(languages=(\"en\", \"sw\"))  # <- Use tuple\n",
        "})\n",
        "\n",
        "# Recast all splits to match NLLB format\n",
        "swahili_clean = DatasetDict({\n",
        "    split: ds.cast(translation_features)\n",
        "    for split, ds in swahili.items()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfjekM2LIvG5",
        "outputId": "cff7b8c2-3dea-46fe-cbe7-58e4316bcdaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'translation': Translation(languages=('en', 'sw'), id=None)}\n"
          ]
        }
      ],
      "source": [
        "print(swahili_clean[\"train\"].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxThvG09sCPX"
      },
      "source": [
        "Everything looks fine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXznBXbhsCM1"
      },
      "source": [
        "## **Save Dataset to Disc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f4ddfd50fea445a79ebf1b2c6fde9e3b",
            "ddebfc3dfa5644bebd9c671140b39abf",
            "aec01f08637646a291d4c200b515fddc",
            "be75263f619f4f408da2647947aa5b5a",
            "dd34e8462cca4b8bb80e1109943e01a0",
            "1a3f72d2533146dcb5a226c69ccb41dc",
            "183acabfadad4e29a7e0fc3d535d4569",
            "403c9c7dee50461ab11c09a7e8a1c1a1",
            "ce78a909b69b401a9d72676b82671dc7",
            "9462e4b81eb74b7a8eb05bc40f411a10",
            "7295f0ee286c4de9a40737a5460805e5"
          ]
        },
        "id": "CrkWvHwksCKn",
        "outputId": "3227a41a-0568-413a-ba69-6574053fe2d6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4ddfd50fea445a79ebf1b2c6fde9e3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/210471 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset saved to: /content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Sw_LaTn/CleanedSwahiliDataset\n"
          ]
        }
      ],
      "source": [
        "save_path = f\"{save_dir}/CleanedSwahiliDataset\"\n",
        "swahili_clean.save_to_disk(save_path)\n",
        "print(f\"âœ… Dataset saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4U3LrnAjhnR"
      },
      "source": [
        "## **Split Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qRopev8jgoQ",
        "outputId": "5c46a89e-d635-43a5-b4d4-2b752ff1dc3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 189423\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 10524\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 10524\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Randomly split dataset into train, validation, and test sets\n",
        "# Swahili_clean is a DatasetDict with only a \"train\" key.\n",
        "# To avoid AttributeError: 'DatasetDict' object has no attribute 'train_test_split', we:\n",
        "\n",
        "\n",
        "# Shuffle and split the dataset since swahili_clean[\"train\"] is a single Dataset\n",
        "shuffled = swahili_clean[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# 90% train, 10% test+val\n",
        "train_testval = shuffled.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# 5% validation, 5% test\n",
        "test_val = train_testval[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "swahili_dataset = DatasetDict({\n",
        "    \"train\": train_testval[\"train\"],\n",
        "    \"validation\": test_val[\"train\"],\n",
        "    \"test\": test_val[\"test\"]\n",
        "})\n",
        "\n",
        "# âœ… Check\n",
        "print(swahili_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6M7yEkAsCGx"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8jUXsqsp8wB"
      },
      "source": [
        "### **Load Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "38dee87a97fa4d80a6eed864cf1a40e0",
            "30f18f4db78d4480bb6b50a63c5884fe",
            "8d4c73451350436789b2fcfb4b88b1c7",
            "af2cb70c2cbe4f8091a43f42a00d8be1",
            "3a4b4fd6dcce41209d8e8d397d3b9271",
            "5166fdd1fbfe41ee9a6863e2dc60b809",
            "19007d971d90464b8f8fd12c392af708",
            "8ed2458592764ccf9dfa22bb2c980283",
            "0da76fd5d299411d8d17a83efac68134",
            "a35d44c029dc45b88e5b544012c05295",
            "481882468a5c4e03b0ea7cf6c00a1eaa",
            "efc0b29da3c04d5bb6d1fd26750a972f",
            "fb329a5ff67f4cc6aca1af838665330b",
            "f1cce23cbbbe4635a892eeef5162d285",
            "4a49f15e71804a91bb21b58584aec2e8",
            "ac8268723ae847ab9ad61cb7bc2beb2b",
            "81167f868c8447cf9b95773d3af47fc1",
            "f477f073248f414b92ceeddc46e94cd7",
            "b5980b3a2ef44fe79c2b2040f473e2a8",
            "0c2fa69233a94dcd8d5bfca76445e1d3",
            "9c2560f937da4f9381ffdf39ac6e399c",
            "38ab170a65964f35bcaecc904ca2da46",
            "06ca8d1af57f48a08848beca84e96348",
            "8bb95e925963487abe7ac45ea80645cf",
            "218fee549ec64e02961a1dd0668ad9cc",
            "eb24b2f4fc114409a5b3dc367b95754c",
            "d02504c0212f4d16bb78a001cc1cce38",
            "02edbb409661415ebea8ab17e5493f54",
            "e5a5298034f9468a89ce4335d0a69acf",
            "70ae890b78e14b3f99041203253069b0",
            "47f66ba7fc63464b8b4830c0b4ead0ca",
            "e19930142a9141369f1ddb1ee8d2b8f3",
            "c8b67df5071d4c65a152ece05eb69dd8",
            "bec6d95d7668428da051b95857eb6e66",
            "47a2408f620a47739cdec0277089d7d3",
            "02dfdd25dd7b47949d9575ce3e49a34a",
            "f96a0f16aef441deaec66f1c59d81285",
            "50997332bea548e8ae355ecee8a62cc2",
            "b3ac44f80dc142a4b2caeb3659d36b4d",
            "3caba21cd2874b17a480f4c4dc325885",
            "3f26060cb31845e1b7380d7eaacc957b",
            "ec74863fcf864f29b9562bf8df8c2f70",
            "9a0212bea54f4bdf838648b76792132a",
            "97ee43cfb61946328994b2e078785344"
          ]
        },
        "id": "AXEK4cgUp8tN",
        "outputId": "1dc97a86-9f9c-4462-c9a1-82b0d5aa8d51"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38dee87a97fa4d80a6eed864cf1a40e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efc0b29da3c04d5bb6d1fd26750a972f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06ca8d1af57f48a08848beca84e96348",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bec6d95d7668428da051b95857eb6e66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "tokenizer.padding_side = \"right\" #  proper for attention mask alignment and decoder positioning for encoder-decoder model like NLLB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emM8wGTBp8p6"
      },
      "source": [
        "### **Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1vp_iykp8K9"
      },
      "outputs": [],
      "source": [
        "# Isolate data splits\n",
        "train_dataset = train_testval[\"train\"]\n",
        "val_dataset = test_val[\"train\"]\n",
        "test_dataset = test_val[\"test\"]\n",
        "\n",
        "\n",
        "# Set the target language for NLLB tokenizer globally\n",
        "tokenizer.src_lang = \"eng_Latn\"\n",
        "tokenizer.tgt_lang = \"swh_Latn\"\n",
        "\n",
        "def preprocess(example):\n",
        "    source = example.get(\"translation\", {}).get(\"en\", None)\n",
        "    target = example.get(\"translation\", {}).get(\"sw\", None)\n",
        "\n",
        "    if not source or not target:\n",
        "        return {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "            }\n",
        "\n",
        "    # Add source language prefix for NLLB-style\n",
        "    input_text = f\">>swh_Latn<< {source}\"\n",
        "\n",
        "    # Tokenize source and target using set lang codes\n",
        "    model_inputs = tokenizer(\n",
        "        input_text,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    # Target tokenization works correctly if tgt_lang is already set\n",
        "    target_inputs = tokenizer(\n",
        "        target,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = target_inputs[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "c83f542b40aa490fb4d0b63cfd077a7e",
            "00cc056daa8d42f39a9adb92b92a667e",
            "b651eaeedec74d6992d7314fc9b0baa1",
            "d03c5864b66b4a15923cfa36d341274d",
            "4661b924d8f44cdd8a4e0ab0f30dbb81",
            "177b1b74d4114c3c8c9c5de5692baa6f",
            "62d176b711a34cdc9dbe1995a11d0f06",
            "8196933905cd4ce5bfca1a0ad59c134a",
            "5ae8246fd2bf4a23be9eecb97347ac1a",
            "79f251de1982486fb415781f8f3225e6",
            "91395a0b9cab49868a15283ee3133b3a",
            "cbcf6471bbd4408db7cbb7dfbf8b9c17",
            "69ef4a3082af4752bfcfcb341aec621b",
            "3879a0a5b67147b2a485081f70bc5316",
            "c9ba6fee844144ccb51db9dc1a6440b0",
            "7e2e954ea6874582ac0d57443831f13d",
            "8cab002fb96641efbc8d480578d6f8a6",
            "e6c14f6369e64580881816f758f02941",
            "e993bdbe7d2f4da7bd73eb7bcbca4dc7",
            "28972add138445789cb66b7598ac5450",
            "a00895760282413a99b7fcc36a00b699",
            "d80a6459e3ee481e92f4d54402302878",
            "1b6adfd501004ba3b6ae33b72c238072",
            "64d0687947964acdba3e4daed73caed1",
            "3df728924e59446c96f6334cac385b33",
            "1cc5db14f6e541cc9f2e1910b92b94c2",
            "4089fc3cc09346df9e7d245584c4754e",
            "30510c1e4bcb4393b0e3c1948597e8bc",
            "87d7da835ce24c18af9b7c1bc882c192",
            "c495f58994e84eec8a3449ed522a5eca",
            "677b2150392744c7a023ea2831756ac2",
            "a10aabc99a3c4543851de38143eb13c3",
            "433b76ee83ba49c5a0c55e91eadd4cbc",
            "7fc7c289a486492d934eacb753c4763a",
            "a4272f5857374adb8a0f5cce85841561",
            "867c80f9474a44798cf30f9d46b098fe",
            "8dd9623a827c4e139c3985257f46f9c5",
            "16dadc9441824813bfd04ea9e5016334",
            "c389f4201c0940039f0402d3755d9dbc",
            "7a45643f61494c169e23816c04dd7b96",
            "7fb19d4152554ac0858d16fefc751a93",
            "45cd16667904434095e8df682928f636",
            "ef82068b76c04504b7d9f03a36eab078",
            "2f5a7b7ec2f041239f75ee9c1fe68c3b",
            "e5e9b0bc6d2c447b8b59044797d0c6b9",
            "b9ae2e2f59bc4793823df4bc4ffb7f95",
            "03c30635ea93492d80603d64fc0c39f4",
            "06196a1e8f234347915ad1ace6cd60d9",
            "18e8d433e6d5486fbf34f7b93f324138",
            "4bdd842303a64cbba39b888488136989",
            "79efaecec1dd4883adde0bdb2646db32",
            "e0aae534ea904ceb8da969d6a985bb4a",
            "c6bc61aefa454ae5a72025cc08d2814e",
            "50b791b322844ef085498d77c7c19550",
            "7e8c8df85c26494cb8e726a9878fe87d",
            "b9ba2e867389459ca61790046384122a",
            "61f0d07bea324c429291115286d83cfa",
            "5b6ee7b6b1af4b2cab4f430da078395b",
            "b4710aadaead43ea82eabbaee8286003",
            "9fce47ecd4f04d7b8f2a4d5ca7042020",
            "2ddf145036344db6b53b119c4d85b107",
            "55e9c1a72b974332964fde665ffb669f",
            "dd53f6d5911c4be5816824d3ca697ce6",
            "a73b7947179f41c18d33955e18b61d9f",
            "0e2e3d7d7494430e9e2dc30eaeb8eeac",
            "e486533bdecf49cba80d9c0a21febf07"
          ]
        },
        "id": "rlpgDiFBXhcp",
        "outputId": "e4187d09-88c6-4d7e-cb02-b3608700f7e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c83f542b40aa490fb4d0b63cfd077a7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train set (num_proc=4):   0%|          | 0/189423 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbcf6471bbd4408db7cbb7dfbf8b9c17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/189423 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b6adfd501004ba3b6ae33b72c238072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing val set (num_proc=4):   0%|          | 0/10524 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fc7c289a486492d934eacb753c4763a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/10524 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5e9b0bc6d2c447b8b59044797d0c6b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing test set (num_proc=4):   0%|          | 0/10524 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9ba2e867389459ca61790046384122a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/10524 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Map Preprocessing on all splits\n",
        "\n",
        "train_tokenized = train_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,  # Optional: use multiple processes\n",
        "    desc=\"Tokenizing train set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)\n",
        "\n",
        "val_tokenized = val_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,\n",
        "    desc=\"Tokenizing val set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)\n",
        "\n",
        "test_tokenized = test_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,\n",
        "    desc=\"Tokenizing test set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MSZL1Txp7_R",
        "outputId": "a0e662c3-3f98-41d0-f140-b7fcb83aad16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [256047, 20545, 256168, 57642, 117, 52605, 117, 14173, 2964, 6399, 6935, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [256047, 11413, 29313, 3895, 248061, 3648, 4885, 6935, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "# Check a sample from tokenized data to confirm tokenization\n",
        "print(train_tokenized[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LcPpKA-Op78N",
        "outputId": "5dc428ab-5e71-4c36-dc9e-7d43cf6a3563"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n{\\n  'input_ids': [...],\\n  'attention_mask': [...],\\n  'labels': [...]\\n}\\n\\n\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The strcture above is interpreted as:\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  'input_ids': [...],\n",
        "  'attention_mask': [...],\n",
        "  'labels': [...]\n",
        "}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSS3vb437RVL",
        "outputId": "95565648-af6a-4f5b-f752-ba6393d4e26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 189423\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM2dqks9p7zG"
      },
      "source": [
        "## **Fine-Tune NLLB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHFNAuXBp7wZ"
      },
      "source": [
        "### **Configure BitsAndBytes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRktXaopp7tQ"
      },
      "outputs": [],
      "source": [
        "# BitsAndBytes parameters\n",
        "################################################################################\n",
        "use_4bit = True # 4-bit precision on base model loading\n",
        "bnb_4bit_compute_dtype = torch.float16 # compute datatype for 4-bit base model\n",
        "bnb_4bit_quant_type = \"nf4\" # quantization type\n",
        "use_nested_quant = False # activate nested quantization for 4-bit base models (double quantization)\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "cc2818c855fd4a5993d6d97e66dd56b4",
            "b92bd0e2d7c64a479df16e2d97b83ce2",
            "51fc6c69bb7e4a02ae739555f5a0f28b",
            "35f23955d332459383ac36f0c406a260",
            "13d0f96a90b04807be3b60424486961a",
            "365508ba89ed4bdfb9ab791f2f10040d",
            "666f9c26f57d4aa2850e0f5d5d085953",
            "f8263c17f3f748828ad74380b716dbf3",
            "a8783f58503941a5a5503229588d7b18",
            "8047b3ad38e6419b951ba99311aca8d0",
            "0a493c1699844cc9bd638d964fd2e696",
            "839fc46defe64199a37c096900e7d7bc",
            "ae80a16861c04eeaa9b806437360b523",
            "d6d91e1e69c94a8faa88db308553ca04",
            "4d350b578ccc4e208952c4078e4834b5",
            "ddf8c5f2bc354f55940f5d898e4204f0",
            "02e02c52b7c24a898c93144e078dd215",
            "7289fba65ea54e1980f834394ea2389f",
            "94f1c3cbf2154a3cab16a85120b0a34a",
            "d01316a981b343298ce1ec2d3cd4533a",
            "ebe1d40288bc472a83983e9939e3177d",
            "9764c90a3c3649fdb4db22bcd5d0796a",
            "84856497f18c452fb654bb43adf11aec",
            "fe625354745249449b79c0e0a4f75763",
            "671b3f7e4803493b9938639cd5c1416e",
            "d2f0ac0c03fb4a5495fad40d4194736f",
            "48419faa379d44d7b27f4dbcdf2099d5",
            "986114372b1641899bd4a0bda973e329",
            "903afd6cf7624ecdb4c0cfce465fdfc5",
            "da7ad7c5834348a7b263f4c81710a397",
            "3c6ca6b4f5ca482bab61372ff8b75821",
            "1008b8ba2cf54e708716999e211a21ac",
            "3193267a62d54daba5ddc09018887404",
            "874023ce65d044c78c93c0b89818e6e4",
            "566edbab0082460785fca321a53e4371",
            "a50d2a38ba084217950c84b9db0d29c0",
            "2518087eb5ea4d81ae07adbfe071b483",
            "41c7e442c7864dae991153d0dd95d3bc",
            "ef76d031d0e44dabaefa7a018dd7d776",
            "ead05e698aa1414ea1d6c2397fb51029",
            "c0a22a6dfb7c4ad290438dde61d253a8",
            "86b8eb47fa7b4a9c9ff583e8fa267032",
            "59e92b65b6564c19a4cfd927d69bf038",
            "d2310fc8013148faa78a82514abac906"
          ]
        },
        "id": "Je9N6UmhsCAA",
        "outputId": "006f4fec-4f6f-4c24-fc25-9dd692a97b2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2818c855fd4a5993d6d97e66dd56b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "839fc46defe64199a37c096900e7d7bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84856497f18c452fb654bb43adf11aec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "874023ce65d044c78c93c0b89818e6e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint,\n",
        "                                              device_map=\"auto\",\n",
        "                                              low_cpu_mem_usage=True,  # Explicitly set to avoid the warning\n",
        "                                              quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S4n34njN6AF"
      },
      "source": [
        "### **Test Model with Zero Shot Inferencing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "jhElREp_N5f8",
        "outputId": "9e1a5721-0163-4d7a-d8c7-3dc9527de48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¤ English: The weather today is very pleasant.\n",
            "ğŸŒ Swahili: Hali ya hewa leo ni nzuri sana.\n",
            "CPU times: user 1.17 s, sys: 943 ms, total: 2.11 s\n",
            "Wall time: 5.15 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# ğŸŒ Define source & target languages (ISO 639-3 codes)\n",
        "src_lang = \"eng_Latn\"\n",
        "tgt_lang = \"swh_Latn\"\n",
        "\n",
        "# âœï¸ Example input sentence in English\n",
        "input_sentence = \"The weather today is very pleasant.\"\n",
        "\n",
        "# ğŸ”¡ Tokenize with language codes\n",
        "inputs = tokenizer(\n",
        "    input_sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# âœ¨ Set language tokens\n",
        "inputs[\"forced_bos_token_id\"] = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "tokenizer.src_lang = src_lang\n",
        "\n",
        "# ğŸ” Run inference\n",
        "with torch.no_grad():\n",
        "    output_tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "# ğŸ—£ï¸ Decode result\n",
        "translated_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
        "print(f\"ğŸ”¤ English: {input_sentence}\")\n",
        "print(f\"ğŸŒ Swahili: {translated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_ByOm0WsB95"
      },
      "source": [
        "### **Setup Model with LoRA (PEFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFqF8EjfsB7k"
      },
      "outputs": [],
      "source": [
        "# Set up LoRA config with target_modules\n",
        "lora_config = LoraConfig(\n",
        "    r = 8,  # Rank of the decomposition\n",
        "    lora_alpha = 32,  # Scaling factor for LoRA updates\n",
        "    lora_dropout = 0.05,  # Dropout rate for LoRA\n",
        "    task_type = TaskType.SEQ_2_SEQ_LM,  # Sequence-to-sequence task\n",
        "    bias = 'none',\n",
        "    target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers (query, value, key, output)\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters to the model\n",
        "model_with_lora = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wrLctjFsB5V",
        "outputId": "1dde4121-5442-4781-cd1f-ff4d4ecf12dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 616,843,264 || trainable%: 0.2869\n"
          ]
        }
      ],
      "source": [
        "# Check trainable parameters\n",
        "model_with_lora.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYBKR9rJsB2B"
      },
      "source": [
        "### **Prepare DataCollator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7fgOW3EsBzf"
      },
      "outputs": [],
      "source": [
        "# Instantiate the data collator for sequence-to-sequence tasks\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer,\n",
        "                                       model = model_with_lora,\n",
        "                                       padding = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Ps9qrwsBx0"
      },
      "source": [
        "### **Define Seq2SeqTrainingArguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEdtHMzusBvZ"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    eval_strategy = \"epoch\",  # Evaluate after every epoch\n",
        "    logging_dir = f\"{save_dir}/logs\",  # Directory for storing logs\n",
        "    logging_strategy = \"steps\",  # Log every N steps\n",
        "    logging_steps = 25,  # Log every 25 steps\n",
        "    save_strategy = \"epoch\",  # Save model after every epoch\n",
        "    save_total_limit = 3,  # Keep only the latest 3 checkpoints\n",
        "    per_device_train_batch_size = 4,  # Batch size per device for training\n",
        "    per_device_eval_batch_size = 4,  # Batch size per device for evaluation\n",
        "    gradient_accumulation_steps = 2,  # Accumulate gradients for 2 steps before updating weights\n",
        "    #num_train_epochs = 1,  # Total number of epochs\n",
        "    max_steps=11000,  # Stop after 15000 total steps\n",
        "    predict_with_generate = True,  # Predict with generate\n",
        "    weight_decay = 0.01,  # Weight decay\n",
        "    lr_scheduler_type = \"linear\",  # Linear learning rate scheduler\n",
        "    optim = \"paged_adamw_32bit\",  # Optimizer to use\n",
        "    learning_rate = 2e-5,  # Initial learning rate\n",
        "    eval_steps = 500, # run validation every 500 steps\n",
        "    fp16 = True,  # Use mixed precision training (not recommended for faster training on GPUs, especially A100 GPUs)\n",
        "    load_best_model_at_end = True,  # Load the best model at the end based on evaluation metric\n",
        "    metric_for_best_model = \"eval_loss\",  # Metric to monitor for the best model (e.g., BLEU score for translation)\n",
        "    greater_is_better = False,  # Higher BLEU metric scores are better\n",
        "    report_to = \"none\",  # Use TensorBoard for logging\n",
        "    disable_tqdm = False,  # Enable or disable tqdm (progress bar)\n",
        "    save_steps = 500,  # Save model checkpoints every 500 steps\n",
        "    label_names = [\"labels\"],  # Name of the label column in the dataset\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IOQD149N4S"
      },
      "source": [
        "### **Compute Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzYXaeC49N0m"
      },
      "outputs": [],
      "source": [
        "# metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# def compute_metrics(eval_preds):\n",
        "#     preds, labels = eval_preds\n",
        "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "#     result = metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "#     return {\"bleu\": result[\"score\"]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC7Kcd3e9NyL"
      },
      "source": [
        "### **Training Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oR-8S_C9Nte"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_with_lora,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized.select(range(min(len(val_tokenized), 2000))),\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics  # compute BLEU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW_SqH4u9Nru"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "34MRpJpr9NhC",
        "outputId": "baaed0d8-7bd4-4796-db67-3babc8f596c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training starts at 1747981464.4833646\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11000/11000 1:26:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.930500</td>\n",
              "      <td>6.877029</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ends at 1747986651.07856\n",
            "Training time: 1h 26m 26s\n"
          ]
        }
      ],
      "source": [
        "# Compute total training time\n",
        "start_time = time.time()\n",
        "print(f\"Training starts at {start_time}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training ends at {end_time}\")\n",
        "\n",
        "total_seconds = end_time - start_time\n",
        "hours = int(total_seconds // 3600)\n",
        "minutes = int((total_seconds % 3600) // 60)\n",
        "seconds = int(total_seconds % 60)\n",
        "\n",
        "print(f\"Training time: {hours}h {minutes}m {seconds}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60KJ7ufX9NfX"
      },
      "source": [
        "## **Save Model and Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-D2ibw39Ndx",
        "outputId": "711cf943-e0ce-43ca-f40d-fe876d0b274b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Sw_LaTn/En-Sw_FT_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Sw_LaTn/En-Sw_FT_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Sw_LaTn/En-Sw_FT_model/sentencepiece.bpe.model',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Sw_LaTn/En-Sw_FT_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Sw_LaTn/En-Sw_FT_model/tokenizer.json')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(f\"{save_dir}/En-Sw_FT_model\")\n",
        "tokenizer.save_pretrained(f\"{save_dir}/En-Sw_FT_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrz9Z-8SHlUY"
      },
      "source": [
        "## **Push to Hugging Face**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2x6BfgWHlQE",
        "outputId": "c07a70ca-71ba-4977-bf7f-c885e4bbdeb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp8l1bHIHmMW",
        "outputId": "48db6c8e-7d85-43ce-8f2b-a7c45dd1f224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "The token `DibiaCorp` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `DibiaCorp`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQOyvQyiJXMq",
        "outputId": "1fcc77b7-8c39-4da0-f30c-47be7928b41d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.1 (from huggingface_hub[hf_xet])\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-xet\n",
            "Successfully installed hf-xet-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub[hf_xet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "1e0fbf38001a4fe690d0c2aa71738e87",
            "a189c55b57f148078904667060f3033e",
            "8c23ccbdc24044c0a4225e5245e035c5",
            "d7353abfea4f49bda8fe0140a3077b5e",
            "19590cd39c134e43ae6391d2f516412c",
            "aa9dbf06da47428f8af89a08b91ad3e4",
            "0abe119bf1334316bd0874a242ff4777",
            "cc49838849114a298ed1ed9b0d3bc48f",
            "728580a7b5944f8fa976c17940c432be",
            "18cd1895e114414b908b59e2566d153f",
            "e7fb44f674bf4148b15ad3b86f74ca01",
            "fd5fec530f664292900a2623a1344d9c",
            "39480556532e4d8fa8a94662c7bd4308",
            "66a40877a90841d79e5d2dca7d38b3d6",
            "7521bbe788174e13af1ab611e89816d1",
            "1453b72f71b94f1098a8906f0d32c4f3",
            "1bb0da9ec00d41fd9adafb91b194a2dd",
            "60df11f40bf643efa9d3757571963ac0",
            "789ee9fc8f8343dda19b964c98ebade4",
            "d821b227484d4d37b7a00033f19b8875",
            "3c795de168a04efcadfb694cffbafa75",
            "0b790a39d0f549608d00a53b9fd1ef66",
            "9e56bf68e7124683ac205c41a04c3674",
            "be5571749515496687ba819d54f8b704",
            "4a39a66a113e468297cebd6eecc1eb7a",
            "66a8273d4af14d01be70669d72f4c48d",
            "aadda35edb4b4c59875ec74f2c25d5d2",
            "8ae76790533e4574ace1f99b22186c81",
            "67da9231574945d2b37eb5f48f5ad40d",
            "07fce0ae33314363ab0a200fe5967e7d",
            "fc3ce6ca6ab34c2eaffc3fc1ca180f0c",
            "2f42f89f0ced4f7fa34f5d7aa706c7f3",
            "517f72c913bc4e0590ef49b4c9e17015",
            "8ba6840916334c918aeb08b7d842d5e6",
            "35d7cafd45aa4e1fbd05f87c292df5b9",
            "e4671d1ebd7e45bba7190c967091715b",
            "da255cd6a42d44c39c39ef7a73c08a1e",
            "806bed3d75bf4cdcb4d4975d48487549",
            "a027dbaee53748dfac012165e1c6de7f",
            "a9c0fb524a7e4e7f8166e3654b1eec16",
            "c3225a774e2b4e55a2485df42dbfd588",
            "ef0027b5ad834501984f05ebd10679b8",
            "f328b6cad5cb41d2942041f6ddbb42c4",
            "fb89e4a10b18411884234101cde51c6e"
          ]
        },
        "id": "A1hgb1e4HlNj",
        "outputId": "3d8e0bcf-351f-4b07-9926-695397aead3c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e0fbf38001a4fe690d0c2aa71738e87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/7.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd5fec530f664292900a2623a1344d9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e56bf68e7124683ac205c41a04c3674",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/32.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ba6840916334c918aeb08b7d842d5e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/drakensberg85/English-Swahili_NLLB_FT_model/commit/d6874d9f992263ba41f5d7fdc1e6835ef4adb838', commit_message='Upload tokenizer', commit_description='', oid='d6874d9f992263ba41f5d7fdc1e6835ef4adb838', pr_url=None, repo_url=RepoUrl('https://huggingface.co/drakensberg85/English-Swahili_NLLB_FT_model', endpoint='https://huggingface.co', repo_type='model', repo_id='drakensberg85/English-Swahili_NLLB_FT_model'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Define repo details\n",
        "repo_name = \"drakensberg85/English-Swahili_NLLB_FT_model\"\n",
        "model_path = f\"{save_dir}/En-Sw_FT_model\"\n",
        "\n",
        "# Upload using transformers\n",
        "AutoModelForSeq2SeqLM.from_pretrained(model_path).push_to_hub(repo_name)\n",
        "AutoTokenizer.from_pretrained(model_path).push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI3p6I-d9NZH"
      },
      "source": [
        "## **TensorBoard Logging and Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcKoftke9NXZ"
      },
      "outputs": [],
      "source": [
        "# writer = SummaryWriter(f\"{save_dir}/logs\")\n",
        "# print(\"Training complete. View metrics using TensorBoard:\")\n",
        "# print(f\"Run this in Colab terminal: tensorboard --logdir={save_dir}/logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZQqrrLb9NTd"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir \"/content/drive/MyDrive/Colab Notebooks/NLLB_600M/En-Yo_LaTn/logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igDlKhBK9NQb"
      },
      "source": [
        "## **Inference Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdRXyhpQw_-i",
        "outputId": "d16b889c-f5d0-44e7-a110-5f70b306f00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: A man runs faster than a car.\n",
            "Swahili: Mwanamume anaendesha kasi kuliko gari.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Use the correct model path\n",
        "model_path = f\"{save_dir}/En-Sw_FT_model\"\n",
        "\n",
        "# Load tokenizer and model from local files\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# Send model to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Example sentence for translation\n",
        "english_sentence = \"A man runs faster than a car.\"\n",
        "source_sentence = f\">>swh_Latn<< {english_sentence}\"\n",
        "\n",
        "# Tokenize input and move to device\n",
        "inputs = tokenizer(source_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate translation\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)\n",
        "\n",
        "# Decode and print translation\n",
        "swahili_translation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"English: {english_sentence}\")\n",
        "print(f\"Swahili: {swahili_translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spj8CD8Uw_7b"
      },
      "source": [
        "## **Incorporating Chainlit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBI_xyl1w_4g"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import chainlit as cl\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_path = f\"{save_dir}/En-Sw_FT_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    await cl.Message(content=\"ğŸ‘‹ Welcome! Type something in English and I'll translate it to Swahili!\").send()\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    input_text = f\">>swh_Latn<< {message.content}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Create an empty Chainlit message to stream into\n",
        "    response = cl.Message(content=\"\")\n",
        "    await response.send()\n",
        "\n",
        "    # Generate tokens step-by-step\n",
        "    output_tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=1,  # Beam search disables streaming behavior\n",
        "        do_sample=False,\n",
        "        output_scores=False,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "    # Stream tokens (you can simulate streaming with a short delay per chunk if needed)\n",
        "    output_text = tokenizer.decode(output_tokens.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Simulate streaming (token-by-token)\n",
        "    for token in output_text.split():\n",
        "        response.content += token + \" \"\n",
        "        await response.update()\n",
        "\n",
        "    # Final update\n",
        "    await response.update()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mycaZysfw_o4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}