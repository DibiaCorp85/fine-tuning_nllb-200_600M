{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DibiaCorp85/fine-tuning_nllb-200_600M/blob/main/_Fine_Tuning_En_Ig_LaTn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46HOKOHDsASy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMTxEDN6sEjk"
      },
      "source": [
        "## **Install Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqHE92AY69XK",
        "outputId": "0c2a3358-a5ce-497e-bda5-11664057c98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet chainlit pyngrok datasets transformers evaluate accelerate peft sacrebleu rouge_score bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrAh2JE3sEf8",
        "outputId": "5400d6f6-669b-47c8-a9bb-e5889ecf8b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.2/491.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade fsspec datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y9wq391sEdk"
      },
      "source": [
        "## **Import Core Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m7ir1BGVsEa_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import random\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from datasets import (load_dataset,\n",
        "                      concatenate_datasets,\n",
        "                      DatasetDict,\n",
        "                      Dataset,\n",
        "                      get_dataset_config_names,\n",
        "                      Features,\n",
        "                      ClassLabel,\n",
        "                      Value,\n",
        "                      Translation\n",
        "                      )\n",
        "\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        ")\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "import getpass\n",
        "from pyngrok import conf, ngrok\n",
        "import subprocess\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB89k0d3sEXI",
        "outputId": "fc6fa115-8550-4538-e3c4-9cb7d6095c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to save model and logs\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn\"\n",
        "os.makedirs(save_dir, exist_ok = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGPT4cP7sEUt"
      },
      "source": [
        "## **Load Datasets**\n",
        "\n",
        "English-\"Language\" datasets are loaded from Hugging Face.\n",
        "\n",
        "The following are the datasets are used:\n",
        "\n",
        "* Opus100 containing the above listed languages paired with English language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wxoGrp5SpN"
      },
      "source": [
        "### **Opus100 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "a274bd1008fb4e058dc35919ad265a4a",
            "ef208aa07a1841a78d0ebd5aeb78d2c5",
            "5a3db95a3dc04d0aac5e198e389ae576",
            "b61562c22427486dbaa6871fd159d6ee",
            "0b613f36313240559c9c2a6ddc11ab9b",
            "f115b6d0994f4ad2996a1a7cde810af6",
            "6a2b606a06e44943987e20b1e267bdbf",
            "5397aa563d58452e80a0f283c76d20f5",
            "65dcf77d67904fc5b38505738bfdd258",
            "7896558fcc5a44998f52e1920db844ea",
            "327d2db2a9e048df863951a596fcfa9f"
          ]
        },
        "id": "yC2sCb91sERW",
        "outputId": "348bb44c-4f91-4500-9f6f-49e5165c6eb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a274bd1008fb4e058dc35919ad265a4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The En-Ig language pair is present in Opus100 dataset:\n",
            " - en-ig (ibo)\n"
          ]
        }
      ],
      "source": [
        "login(\"hf access token key\")\n",
        "\n",
        "# List of desired target languages ISO codes(to pair with English Language)\n",
        "target_language = {\"ig\" : \"ibo\"}\n",
        "\n",
        "source_language = \"en\" # Fixed source language\n",
        "\n",
        "desired_pairs = [f\"{source_language}-{tgt}\" for tgt in target_language]\n",
        "\n",
        "# Fetch all configurations from Opus100\n",
        "available_configs = get_dataset_config_names(\"opus100\")\n",
        "\n",
        "# Filter those that exist in Opus100\n",
        "present_pairs = [pair for pair in desired_pairs if pair in available_configs]\n",
        "missing_pairs = [pair for pair in desired_pairs if pair not in available_configs]\n",
        "\n",
        "# Print results\n",
        "print(\" The En-Ig language pair is present in Opus100 dataset:\")\n",
        "for pair in present_pairs:\n",
        "    print(f\" - {pair} ({target_language[pair.split('-')[1]]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMTkEkrisEPK"
      },
      "source": [
        "#### **Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "25280c80fa604a5c99c37979632e22fd",
            "805daeed921b40d9a3edaf3693491809",
            "d678370ec82b4b098ecff35c88f38af3",
            "5a47ffe7187a48f1baeb534e3d4e119b",
            "a7805f8eefdf4e49b0f108291f56d6ee",
            "58475996937d4672ab7c86bd951fddc8",
            "3f1bb24d7b9c473585e2f5d29d8a358d",
            "effdbd3476c349168d665be22af59cdb",
            "ef2a523d5da749cc9e07b75b4fc4f2a3",
            "486ead8e3b144fc1978a85a56e6496d0",
            "466c0b8ac8654172ab484b1757a59d04",
            "f14db724c25a48d3afb947d5c9b49d54",
            "1951f0af2e6a46e19f27feaef7c5b46a",
            "b7ef2240379641a38f82232b4fc7e2b4",
            "84ea5c681d0d416c9dd9888038de805f",
            "4e1d2197a63946e8b652353fae211195",
            "ee1bf992e8484dd8acca751749d8bd4b",
            "78e06797997542c3bbddb7ccdd04035c",
            "8b83bd38dbde43359015358817b2b55b",
            "3d07770c913345beb41b1c0d7721aca7",
            "b72e2619c35e43e7bcfcbacc70d8baa1",
            "4bfafb4a2e924ccc916bd0387c619e6c",
            "88a538cc2c7d441a8cf7916491776f7c",
            "8a0bd945f5f648669bfa6ca72e1994be",
            "494095f027294285965441ecc345e659",
            "6264e86d03254cefb7fe7393201c6d4e",
            "6f0e08c3d3a0418d8d64f715120dc96c",
            "cdaeb008833f4fdfa6e8b2dff69ded73",
            "e1c2556dbff94a20af5e9d9b45a3fa9d",
            "dad4e3e349de42489ee283bec20e0a6d",
            "39d33b953f624c69a002afa70a98c4ca",
            "f2a7e1d8428f4784a3565a5958313113",
            "29cb15ff595844dd9a58fdf59b6ce159",
            "afa415abc61c41b0b0d7a975fec2733c",
            "d287cd3705b245ca9e82e8b35d72988d",
            "a708dc184581447686cab3d366ae3715",
            "ef41098adadc44df99f1726efdf1fcfc",
            "ade70c11dcd64a1cbcc17e85bd676918",
            "4d1cf66dba124cbe8ae6fb2540ecfdc5",
            "f2f6e41c29354f0f822fcd7c128f89d5",
            "afa05d9e7bed4218997d3d882cd67a51",
            "cb08483d74f74aeba4c0d0d27aecc83e",
            "fd13b14b76f845d9af4c974b6d74955f",
            "1a3dbe1020a84d0a81859788cf9cbb92",
            "7bf8d05c6853465c93f53f8f269e40bc",
            "cf6b4bb29f0e47ceb9e1982c08bce06e",
            "c0a1356adf8043b7a63c34cc235c30e3",
            "7fc36535884a4114a1b6b2809db0a992",
            "3ffa5fadb0a9449ca9b54f22c6efbfeb",
            "9b666f8a20434631972e43977d733919",
            "692ef554e3ce4272b263c1b7a70eb3ab",
            "2372275d23204116a5132c483e34a248",
            "a7e7e1f37da34794bab8c65fcf07a988",
            "c82174178c7e4792b157ad37d1918af5",
            "125657c729cf4778af3803d512fe8f0c",
            "ee935bc12ad949fc929604b6d4d27b88",
            "589456d1a42c4b57ae7f5a72c928a835",
            "cc795eea91154e98b9ffbdc21c302408",
            "0a96c43bb9f448298a5730171a01272b",
            "e829dec836ec4f8ab7ae854d67a4754b",
            "b50a8b61b5214b469d1fd6b56c3fb6c3",
            "ec08e8f4a29c4371a37725724d3899b6",
            "eba0d2681ddb40e6aa3d340b29e66817",
            "41006384703546319ab6a67a9dcaad79",
            "697d67f9c8034f988e55ba8700a549ac",
            "112390ef87c3439898283464c74a5394"
          ]
        },
        "id": "RLOJlAoOsEM2",
        "outputId": "7db5cef0-b009-466d-8b34-ab7acb7048e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25280c80fa604a5c99c37979632e22fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/44.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f14db724c25a48d3afb947d5c9b49d54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/770k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a538cc2c7d441a8cf7916491776f7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/45.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afa415abc61c41b0b0d7a975fec2733c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bf8d05c6853465c93f53f8f269e40bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/18415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee935bc12ad949fc929604b6d4d27b88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English-Igbo language pair downloaded!\n"
          ]
        }
      ],
      "source": [
        "# English-Igbo\n",
        "selected_language = [\"ig\"]\n",
        "\n",
        "if \"ig\" in selected_language:\n",
        "    try:\n",
        "        opus_en_ig = load_dataset(\"opus100\", \"en-ig\")\n",
        "        print(\"English-Igbo language pair downloaded!\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to download English-Igbo:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBRXB-QEsECn"
      },
      "source": [
        "## **Qualitative and Quantitative Examination of Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sP_fn47_KhD"
      },
      "source": [
        "The following is a check-list to examine each dataset:\n",
        "\n",
        "1. Splits\n",
        "2. Features/columns\n",
        "3. Format\n",
        "4. Missing row\n",
        "5. Check internal usaga of language pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw8QvOLuCwW4"
      },
      "source": [
        "### **Splits Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpROvuCBAZX1"
      },
      "outputs": [],
      "source": [
        "# Check splits\n",
        "def print_split_info(name, dataset):\n",
        "    print(f\"\\nğŸ“Š Dataset: {name}\")\n",
        "\n",
        "    if isinstance(dataset, DatasetDict):\n",
        "        for split_name, split in dataset.items():\n",
        "            print(f\"  â¤ Split: {split_name} | Rows: {split.num_rows}\")\n",
        "    elif isinstance(dataset, Dataset):\n",
        "        print(f\"  â¤ Single split | Rows: {dataset.num_rows}\")\n",
        "    else:\n",
        "        print(\"âŒ Unrecognized dataset type.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXGZYUTUAfLg",
        "outputId": "cd731747-c89e-4be2-a975-077c36be9ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š Dataset: Opus100: English-Igbo Language Pair\n",
            "  â¤ Split: test | Rows: 1843\n",
            "  â¤ Split: train | Rows: 18415\n",
            "  â¤ Split: validation | Rows: 1843\n"
          ]
        }
      ],
      "source": [
        "print_split_info(\"Opus100: English-Igbo Language Pair\", opus_en_ig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339LSrt-CzqX"
      },
      "source": [
        "### **Features/Columns/Schema Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8JngOza_9R1"
      },
      "outputs": [],
      "source": [
        "# Check schema/column names using just the train split\n",
        "def print_dataset_features(datasets_with_names):\n",
        "    \"\"\"\n",
        "    Prints the .features of multiple Hugging Face datasets with names.\n",
        "\n",
        "    Args:\n",
        "        datasets_with_names (list of tuples): List of (dataset, name) pairs.\n",
        "    \"\"\"\n",
        "    for dataset, name in datasets_with_names:\n",
        "        print(f\"\\nğŸ“˜ Features for: {name}\")\n",
        "        print(dataset.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2PbhGxesD-O",
        "outputId": "f92e2beb-1c6f-407f-e3ae-846d3522d014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“˜ Features for: Opus EN-IG\n",
            "{'translation': Translation(languages=['en', 'ig'], id=None)}\n"
          ]
        }
      ],
      "source": [
        "print_dataset_features([\n",
        "    (opus_en_ig['train'], \"Opus EN-IG\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg6fv3ljsD7-"
      },
      "source": [
        "### **NLLB-Format-Compatibility Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aBzZIbQosD6b",
        "outputId": "3c02f8b0-dc4d-49ec-e1f8-f2caad835cdb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n{\\n  \"translation\": {\\n    \"source language\": \"source sentence\",\\n    \"target language\": \"target sentence\"\\n  }\\n}\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The NLLB model expects the following format:\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  \"translation\": {\n",
        "    \"source language\": \"source sentence\",\n",
        "    \"target language\": \"target sentence\"\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7enH7Gev75-1"
      },
      "source": [
        "#### **Recast Dataset to NLLB Format**\n",
        "\n",
        "To recast your dataset to the NLLB format, you need to ensure two key things:\n",
        "\n",
        "1. The translation feature uses a tuple of language codes, not a list.\n",
        "\n",
        "2. This format aligns with what the NLLB tokenizer expects, especially for multilingual training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "380a8f6318694256977389f7ee05c4d6",
            "486ca0944c284b4882d230cde59b05b6",
            "757e4cb1a3e74d16ac655b795e839ac7",
            "a4efa0cd3fae4b1a986bc30019e7f3ba",
            "f61fd668119a44b19b36ba50647fabae",
            "d83df3fb6cb141719789af30244882be",
            "339482630535430d9c8923b2d6b63fd7",
            "d196bb545fce428da60684903be71481",
            "23c9262e60c74a08b641f372418f4d06",
            "bbc27b47a90640be903590420736d104",
            "9c49acd141cb49de9d9774bda3550fdd",
            "5c71f9d8803a47aca91ba30b4d204053",
            "a4e53191bef245e986c2f1c65fd2723d",
            "4f346a92f4714da0a003529e6998654c",
            "8c97a90e66664e6597dac33d89051b17",
            "1c83e3140bd24fcba508c6f7f29e3983",
            "2a850f6e62cc4ac39f8004bf51c6f8f7",
            "eb4d414e680344e4bb215cb126cd14a5",
            "c3b7cd71322a444c869f8defb7e4f819",
            "b16639d397b14da49b694fdca9d448d2",
            "cc56fbdd6302459b82619d9a7dd43998",
            "1a893c0fd1764a1788b558ba44768edc",
            "eff8283f20444a669d3cc428d86099b3",
            "be0e619b3df343ae9b3f4942ee8bd5c4",
            "8bea6634d05746f8a62a9098a4221f9f",
            "545e36f4eb7c4a7b96f3468e1890e2dc",
            "9640091dae9e429db9e59c3697ad4d56",
            "21ca0fadcc10404c8740f7f9924e00bc",
            "4c52b23a26ab4671b060daa228d8ee7c",
            "82486a03862e4882bbcb1c83dcb4d3fc",
            "c56cc3b307bc4c579a17f27281435343",
            "9020a3bf41454b0a87031757f2ead474",
            "a39d3d4aee9c4244aa40c0764a3fe1e5"
          ]
        },
        "id": "Iw0jsVPZ8H-o",
        "outputId": "32f90ad7-2973-4582-e547-0252e7f4cf58"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "380a8f6318694256977389f7ee05c4d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c71f9d8803a47aca91ba30b4d204053",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/18415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eff8283f20444a669d3cc428d86099b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the correct NLLB-style features with a tuple\n",
        "translation_features = Features({\n",
        "    \"translation\": Translation(languages=(\"en\", \"ig\"))  # <- Use tuple\n",
        "})\n",
        "\n",
        "# Recast all splits to match NLLB format\n",
        "opus_en_ig = DatasetDict({\n",
        "    split: ds.cast(translation_features)\n",
        "    for split, ds in opus_en_ig.items()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJrOoZEF9OMn",
        "outputId": "83fdce39-6e74-40f0-9f99-b0302c84d96d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'translation': Translation(languages=('en', 'ig'), id=None)}\n"
          ]
        }
      ],
      "source": [
        "# Verify format\n",
        "print(opus_en_ig[\"train\"].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNijpKsUsDyF"
      },
      "source": [
        "### **Check Missing Row**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRfQs3GgsDui"
      },
      "outputs": [],
      "source": [
        "def check_missing_rows_all_splits(dataset_dict, name, src_lang=None, tgt_lang=None):\n",
        "    \"\"\"\n",
        "    Checks for missing or invalid rows across all splits in a DatasetDict.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict (DatasetDict): The dataset with multiple splits.\n",
        "        name (str): Dataset name for reporting.\n",
        "        src_lang (str): Source language key (e.g., 'en').\n",
        "        tgt_lang (str): Target language key (e.g., 'ig').\n",
        "    \"\"\"\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        total = len(split_dataset)\n",
        "        missing = 0\n",
        "\n",
        "        for row in split_dataset:\n",
        "            try:\n",
        "                if \"translation\" in row:\n",
        "                    trans = row[\"translation\"]\n",
        "                    src = trans.get(src_lang, \"\").strip() if src_lang else \"\"\n",
        "                    tgt = trans.get(tgt_lang, \"\").strip() if tgt_lang else \"\"\n",
        "                else:\n",
        "                    src = row.get(src_lang, \"\").strip()\n",
        "                    tgt = row.get(tgt_lang, \"\").strip()\n",
        "\n",
        "                if not src or not tgt or len(src) <= 1 or len(tgt) <= 1:\n",
        "                    missing += 1\n",
        "            except Exception:\n",
        "                missing += 1\n",
        "\n",
        "        print(f\"ğŸ” {name} ({split_name}): {missing} missing / {total} total rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn8hyABDsDsc",
        "outputId": "b9087c60-af88-49bf-d028-c58e0bf3d0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Opus EN-IG (test): 1843 missing / 1843 total rows\n",
            "ğŸ” Opus EN-IG (train): 18415 missing / 18415 total rows\n",
            "ğŸ” Opus EN-IG (validation): 1843 missing / 1843 total rows\n"
          ]
        }
      ],
      "source": [
        "# Run checks\n",
        "check_missing_rows_all_splits(opus_en_ig, \"Opus EN-IG\", src_lang=\"en\", tgt_lang=\"ig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ignb7_4wsDJu"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y__pI6DfsC2s"
      },
      "source": [
        "### **Clear Missing Rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0DyAamasC0J"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "def clean_missing_rows(dataset_dict, src_lang=None, tgt_lang=None):\n",
        "    \"\"\"\n",
        "    Removes missing/invalid rows across all splits in a DatasetDict.\n",
        "\n",
        "    Args:\n",
        "        dataset_dict (DatasetDict): The dataset with splits (e.g. train, test, validation).\n",
        "        src_lang (str): Source language key (e.g., 'en').\n",
        "        tgt_lang (str): Target language key (e.g., 'ig').\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict: Cleaned dataset with bad rows removed.\n",
        "    \"\"\"\n",
        "    cleaned_splits = {}\n",
        "\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        def is_valid(row):\n",
        "            try:\n",
        "                if \"translation\" in row:\n",
        "                    src = row[\"translation\"].get(src_lang, \"\").strip()\n",
        "                    tgt = row[\"translation\"].get(tgt_lang, \"\").strip()\n",
        "                else:\n",
        "                    src = row.get(src_lang, \"\").strip()\n",
        "                    tgt = row.get(tgt_lang, \"\").strip()\n",
        "                return bool(src and tgt and len(src) > 1 and len(tgt) > 1)\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "        print(f\"ğŸ§¹ Cleaning split: {split_name}...\")\n",
        "        cleaned_split = split_dataset.filter(is_valid)\n",
        "        cleaned_splits[split_name] = cleaned_split\n",
        "        print(f\"âœ… {len(cleaned_split)} rows retained from {len(split_dataset)}\")\n",
        "\n",
        "    return DatasetDict(cleaned_splits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "f240848662334bdb98d17ed7db727a80",
            "50e042f0ef814e39bded21b3f57095fe",
            "4fa799229e2f44d18f91e0fe30aa3dee",
            "527be6ffc16348be864f56e85331d174",
            "677bd2acdae1457b894ba5118c913323",
            "1cb2cb1d87c148c38058accae5badd79",
            "3fa9bfe7ed8249e787156bfbae8eae91",
            "d2fc32ed122d46528e74a3c7d8797cfa",
            "153b0b0d16b249ebbd2d12627ad9830f",
            "84c2f1ae21c44acc87ec9a096d5ea76c",
            "b2fd4ad5406c490793553d076f07c01f",
            "420ea28f97574af28ac48b82ead3ac5c",
            "1f5da340d4494fb8949722ad3ab90294",
            "a60a2359dd364adfb804e6ab557ac6a4",
            "4796310635f14c83951c1fb7d9dde43d",
            "3fe68dbed9324f02a5ee104d47a1e179",
            "a37133372ec84bc599959f68751718b2",
            "f50e82b79c4f46daae9a5716d11255a2",
            "11b64eea41164353a1fb280014b072c4",
            "dac9a6ffc8354230abbcfb3b30098444",
            "b5b1e1d4eaa24e9fb6c64d24524f7659",
            "2474d9c15745418bb22981915b3d2c6b",
            "13afb4222238417d9a07d0d21873b66d",
            "757fe41ea0014510898bbdf602819a79",
            "e43d9b18a6d6427eba12a338e5ac790c",
            "5677c2131c4c41a9a2a82048a9a3beda",
            "38add576582b407e879626ee3397e685",
            "57bba89c3c7c42219fe65d3b3ef63264",
            "5446e53fd2124f6dae74c61d3cb3c840",
            "a862127365d043ad8c4d2348ca4fc01e",
            "9b1a0630abf74c8199ea56671c2c53a6",
            "6d451370dc5e429692eb3fdf6fb4db72",
            "7320b96e62bb4083994e25d83d93b401"
          ]
        },
        "id": "KSbD4cVFsCxX",
        "outputId": "ca6f8a2d-2a0c-4064-f6f5-f8c79ea883a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ Cleaning split: test...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f240848662334bdb98d17ed7db727a80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… 1839 rows retained from 1843\n",
            "ğŸ§¹ Cleaning split: train...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "420ea28f97574af28ac48b82ead3ac5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/18415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… 18406 rows retained from 18415\n",
            "ğŸ§¹ Cleaning split: validation...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13afb4222238417d9a07d0d21873b66d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… 1835 rows retained from 1843\n"
          ]
        }
      ],
      "source": [
        "opus_en_ig_clean = clean_missing_rows(opus_en_ig, src_lang=\"en\", tgt_lang=\"ig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghLvHPs7IavA"
      },
      "source": [
        "### **Standardize Feature Schema: Confirmation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "137fccce845a4390862507728f77ad96",
            "5ad9d0b340bb46378e129fdaf060289c",
            "70c884ae1e274b699c9a9ed97abd5d5d",
            "0a33ed46334e470e9051d0f31e7ef4b2",
            "0d6877a20b7d423d9b4b19f2c3600602",
            "2c01f884775c46c19b126d576d19aef7",
            "6e330055ef5e4bf58ba1439128dcad26",
            "6022f00dda1546ffa354dca3f9160c00",
            "d4b51ff24541464aa6e3494230158ede",
            "933c2ca86d0a42be968defceb0fd7487",
            "0a9d2234e520498694cd09b4e6b9c42b",
            "7c7997413b42474ab8f6c92dd787e91a",
            "e5617ebe400047e5af2b237db3c00126",
            "bc4a75c8bb404640ba35341d81ff91a1",
            "a2547b2024fa4c8b817c217eb8e3eb56",
            "68e681477dc946b4a4257fe61e2f5746",
            "cbbd91c5b11f485594d99a99ae77c24d",
            "efac792308134bffa3d313db9ac9c34d",
            "30f306e9a9534d4fbcfce9716df83ffc",
            "698dbc20a10142b2b682d748cdda6855",
            "9897813bfbdd4cefb9b38fecb22c3966",
            "813c034c1a024fc588addbd46ffe886f",
            "3f4e7acd8bd64882841082ea5e407b7b",
            "802fc26dca444d1db4fc20f7d1441db6",
            "59fdb6bedd3940a28b73f55b3547ee69",
            "0c31ccf91bcd483391862736e22f7c6b",
            "424363b4fd54460fa78d6ba93f75b018",
            "22729c8adc024303be3d0ad23133fa7c",
            "46068d23c8864b3691080596f0198882",
            "8974a4635572415b898cb5aa05db7a76",
            "2bc87ecd7f24426ea6a09b92f4d62653",
            "ef436c4e727c4a1fb8a15645236a475d",
            "79d334841a45437fb8dd86edf5e03a09"
          ]
        },
        "id": "WFJaXeoxIk28",
        "outputId": "fc9a6cc3-dacf-47f4-b5f6-ae7544c48b2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "137fccce845a4390862507728f77ad96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c7997413b42474ab8f6c92dd787e91a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/18415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f4e7acd8bd64882841082ea5e407b7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'translation': Translation(languages=('en', 'ig'), id=None)}\n"
          ]
        }
      ],
      "source": [
        "# Define the correct NLLB-style features with a tuple\n",
        "translation_features = Features({\n",
        "    \"translation\": Translation(languages=(\"en\", \"ig\"))  # <- Use tuple\n",
        "})\n",
        "\n",
        "# Recast all splits to match NLLB format\n",
        "opus_en_ig_clean = DatasetDict({\n",
        "    split: ds.cast(translation_features)\n",
        "    for split, ds in opus_en_ig.items()\n",
        "})\n",
        "\n",
        "print(opus_en_ig_clean[\"train\"].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxThvG09sCPX"
      },
      "source": [
        "Everything looks fine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXznBXbhsCM1"
      },
      "source": [
        "## **Save Dataset to Disc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "45a68aec21b34b29a9b42541193195f7",
            "35bbf6edba8f4017ad6ca12c65b4b0c7",
            "fab403f00e4b46d3a595ed51ea2706e1",
            "8f2037bdfc324c5f93ac3ad50f5ca014",
            "1fcf848734b24ba7894a5c9c5077e1d6",
            "68439f51f8cb4e2a86815a91f06341d5",
            "fabaff16e979479d9f6c273e47c156ae",
            "440dc70e3f104864abf623d4cc760ca0",
            "5e895af30faa4e1bb0ebe5dc72ad4a43",
            "4ab77f96bf7144f2b911693a03cf5fbd",
            "3a13b77581ec4e4aae942e2abc8f39a2",
            "cafdddbe91584fee9a515a2467ef7c69",
            "b7243a9d14664bbe8c3ca783cdf0e481",
            "d7879fcc00d94a0ea4ff0d3edb251d4b",
            "8d2b209acc81460e96f2546670c174a8",
            "1ce023a8824345a9aeb0ac4b20524de8",
            "95b8c17f6f16490cab812fa6190cde82",
            "53a38b18edee49a497dc4cd83e8bc058",
            "995a886fa23a4f329b4d40fcf431f100",
            "678636320b8c47a1ac6aee70c01c1c85",
            "000dbdebe236441582f6cba591db84bb",
            "b69d1821307b49cc920e8314a52622d2",
            "49fbb1f52b9044468c59164e961c470f",
            "32af22615282407c9d01fd58fe1f2f15",
            "e187ff1d1772489f9e4a7bd2d82d2184",
            "a099ef81bd4444f48f21f44b234a811d",
            "5a61578663bb44ccab17a5acf9fab951",
            "31465be6d221471a975302edb69d241e",
            "6a4b68bbaf9d4fe6aae2a06c4004e8c4",
            "8d9b5c05ae3a4222a1d434339ea2c407",
            "e03389a2eca343d39d3fd92ebd1199b4",
            "207d49c11c6f46b6864e4de8c55faa5b",
            "5f89d349fae54cffb8997c19d0d571d8"
          ]
        },
        "id": "CrkWvHwksCKn",
        "outputId": "ad0833e5-7ec4-47b8-ae34-77b6d19d6bc2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45a68aec21b34b29a9b42541193195f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cafdddbe91584fee9a515a2467ef7c69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/18415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49fbb1f52b9044468c59164e961c470f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset saved to: /content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn/CleanedIgboDataset\n"
          ]
        }
      ],
      "source": [
        "save_path = f\"{save_dir}/CleanedIgboDataset\"\n",
        "opus_en_ig_clean.save_to_disk(save_path)\n",
        "print(f\"âœ… Dataset saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6M7yEkAsCGx"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8jUXsqsp8wB"
      },
      "source": [
        "### **Load Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "83a2813875b14ccfadffe045fb236cb7",
            "42a3ecbe940d4b62a29e378a9e64b239",
            "2febde7b6d034dbfbfb065ca00650daf",
            "042b974135f64af09e742c0475bdb1eb",
            "792dfa981af844fba5806c0f1d9fb4e5",
            "a3885a9fbc8040608ebe3e0e9727306f",
            "137f8f3e97eb442cbbeff50df28a1c26",
            "d6191e0e4ad44ac9898f0e11c12a3d51",
            "7ab1f7ae850f489aa9566dd90d86c0b2",
            "83885d7c60684ca088acd36b90bd8956",
            "a0a4fb166d374774a58f1885f68c0e7f",
            "dedc87937c2549958dba63368a68ede6",
            "bff32be0320c4174b28eede2b9304c83",
            "a27d4fb461e24fb894538fb20c09a490",
            "fae7e76b25b3415f955fcbe0538e81c6",
            "ec5f61e8d9fb4296b6b2147424cf4462",
            "6336eaf4ab704197be0d0f1df9a99f23",
            "73b2230e7eb64e8399044c5cc953c02c",
            "76b53df48f9b4393a5574d2c6969cf99",
            "db599d3cc2b44386a35b31668cae9cae",
            "3171e33381dc46f1b05b9c218096282d",
            "f2fe0ebf6b22426589da71d5fcafd0fc",
            "89ec2dca2dd64874be97e180dce695e6",
            "afcdbae1bfec4837b61fcbc6d579d6c5",
            "672db39fc0824bedaf642af905ddfe67",
            "2fbc149a6cac4ffd95a06d43ae9c3275",
            "967c65d5881f470a8f27aa093309342e",
            "9957475d78114abd8373308965bccdb1",
            "463d6159454740bcb394084908c6d4ba",
            "0cefcfa58d9a47adbaac32ec74abadbd",
            "e097371c9df8414b887a1cfc9d6ebcd9",
            "671ab2c5dfaf406896a54484ca77380e",
            "899cbdac2cc34d54833612bf58849af8",
            "02f6dd320d4e4499a6965a9287d36abd",
            "a0286a79c6364f5e8d91dbf8b3fe3fc9",
            "8a533ad1b5054314afdce16eadaad3c7",
            "31c4cb72c1e045c897f45772bb17a4ff",
            "9b02cd17c3aa4d9396fb17e354e37fb4",
            "d37d11706bd24687bf2aa667cbc6afe9",
            "f5a66e7746944593b78a2c4987937d98",
            "796fb1378f7e481d909a508999299dea",
            "41606fff26444cb2b43a20f4bffa231b",
            "927e0e631580428a8a0c9b46eef03e02",
            "5ec6d198c01b4892ad5603db26eab362"
          ]
        },
        "id": "AXEK4cgUp8tN",
        "outputId": "8c1a5e6e-a8a3-45b1-f578-f35bd5538416"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83a2813875b14ccfadffe045fb236cb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dedc87937c2549958dba63368a68ede6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89ec2dca2dd64874be97e180dce695e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02f6dd320d4e4499a6965a9287d36abd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "tokenizer.padding_side = \"right\" #  proper for attention mask alignment and decoder positioning for encoder-decoder model like NLLB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emM8wGTBp8p6"
      },
      "source": [
        "### **Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1vp_iykp8K9"
      },
      "outputs": [],
      "source": [
        "opus_dataset = opus_en_ig_clean\n",
        "\n",
        "# Isolate data splits\n",
        "train_dataset = opus_dataset[\"train\"]\n",
        "val_dataset = opus_dataset[\"validation\"]\n",
        "test_dataset = opus_dataset[\"test\"]\n",
        "\n",
        "\n",
        "# Set the target language for NLLB tokenizer globally\n",
        "tokenizer.src_lang = \"eng_Latn\"\n",
        "tokenizer.tgt_lang = \"ibo_Latn\"\n",
        "\n",
        "def preprocess(example):\n",
        "    source = example.get(\"translation\", {}).get(\"en\", None)\n",
        "    target = example.get(\"translation\", {}).get(\"ig\", None)\n",
        "\n",
        "    if not source or not target:\n",
        "        return {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "            }\n",
        "\n",
        "    # Add source language prefix for NLLB-style\n",
        "    input_text = f\">>ibo_Latn<< {source}\"\n",
        "\n",
        "    # Tokenize source and target using set lang codes\n",
        "    model_inputs = tokenizer(\n",
        "        input_text,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    target_inputs = tokenizer(\n",
        "        target,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = target_inputs[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "ea8761c9da8541138affb5f8e4f5cbed",
            "c2776e500668498daa0e8788b8c58f98",
            "a88482f40416426a98b60b72deac241c",
            "84bdb6232f094105acb3af5e8118a391",
            "03ec977f102c4344ac6c078df4f671a3",
            "e64dd7799bf24e9aa134d5f5de2eb184",
            "e06d4f08fe6749f9a154ccc2bf59901a",
            "72da2e463daf43c9be705f3e67e6c9f4",
            "13aef9108b474df895231185b1c650dc",
            "52848f8d25614f0f92f76a891374e3ee",
            "aa2173913c8a4624af1c8d2b9cbf2960",
            "2c8195e7e6d041138200e6db7f512a68",
            "ecbccb71df2e472096e420d2e47857bc",
            "b95c021513b64b188104ecb9d4ac2e71",
            "99b39f2f5d8340b094de0ebc57e299f0",
            "d7a67506d0e842568b484f76b5083bfd",
            "545e92ae0fe44daea981410495bbbbd3",
            "48eb611007194873aa76f1ea92e24806",
            "91ad356b0b664c6fbc8d9919c778c3c6",
            "0117287011aa4477b0ba779134da02c4",
            "ad5850b3972d4505849c68fccfdc39d9",
            "e4449ba9c3b74357bddfc18919a1c741",
            "f4a4fdb2e38047adb26b71b84d4e9893",
            "c662fe1a7a9644da915811b909f818f6",
            "f03a0b42c83441209832421019762239",
            "0fb86a9f9193448d94d54415883fd5a2",
            "0fc7de381c4a41fa86f61306bcd47707",
            "bca1f37e7f0c4c819ad864081650200f",
            "3d691a8e832c4b31a566a97e6c3bc872",
            "776dda21751349cb914d9914c65eb0a5",
            "6bd1bd25b32d439f9b275f824c0f2e71",
            "dbfe754cb5f1404ea70da9a4b68b6589",
            "f98f1541c8084843b1e357458d431d56",
            "fd5233accba946beaa215dc6d0d6f102",
            "9078e21c248d477faf68f6bef3b313a4",
            "b9241bbe4d8b42b8a5048d22456bcf28",
            "c2e7eda8931f44d1a5280c5b695d59bb",
            "3229393d93fd41f0b81a478511de85f1",
            "fe3efc53511249bfa53dded5988b019b",
            "c5b4da596b514d94857e7c7e4be79dc6",
            "eb9a2a07ee444516bf13139d2fc73048",
            "d62ccf13b88845f7ae81b63f5818ea69",
            "e9be00dd07924d94b158492db11ea63f",
            "a4ad0535d8dc4de0bfcf4c313f13a632",
            "deb62e97bbf740618880712912a834dd",
            "976e0509427441f397095c8a42f9029c",
            "55f49b3bd1624ab091dd6bbbdbd45f0c",
            "9b179d7c16514892a92f5dc1e40ccea5",
            "290b8b16fbd644649f21f78080f33d7c",
            "8f217a299eca46ad9799c6c52bb1947a",
            "81f9729087d84db1909d48e3f5825060",
            "c85a0c450f3142528611cbc5453fa004",
            "2fde45e6122d4113a81d3a04cbddfd1f",
            "37f72380b896484cad8a6e1aab1efaed",
            "4b178025e19f4c35914083a16bf87be3",
            "de1e130c28a842d99eb0c2b33e4671c5",
            "07ad045e32d2417bbf04eb17d80b2109",
            "a0248b45e6d64a3b9df69d66f4911589",
            "c3a7fbfff0d04a1abf1cf767401c98cb",
            "b798c03b0b194e70b6ab8f287423c4a6",
            "292fa721f49d4f72832f7d4fa949b837",
            "f9bc7eb2d90f432497274212e0982a3f",
            "0766a0095bcc4098acd3d239c8a8b638",
            "2f6baed8649149b0976e572832b8ddec",
            "76f71072152743a4b7e67ec7b3c1cac4",
            "cd6640a642ce424789bd54d6030ff6ee"
          ]
        },
        "id": "rlpgDiFBXhcp",
        "outputId": "ee8b632c-10a3-4969-d7d8-9d1583474f3e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea8761c9da8541138affb5f8e4f5cbed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train set (num_proc=4):   0%|          | 0/18415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c8195e7e6d041138200e6db7f512a68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/18415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4a4fdb2e38047adb26b71b84d4e9893",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing val set (num_proc=4):   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd5233accba946beaa215dc6d0d6f102",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deb62e97bbf740618880712912a834dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing test set (num_proc=4):   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de1e130c28a842d99eb0c2b33e4671c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1843 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Map Preprocessing on all splits\n",
        "\n",
        "train_tokenized = train_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,  # Optional: use multiple processes\n",
        "    desc=\"Tokenizing train set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)\n",
        "\n",
        "val_tokenized = val_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,\n",
        "    desc=\"Tokenizing val set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)\n",
        "\n",
        "test_tokenized = test_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,\n",
        "    desc=\"Tokenizing test set\"\n",
        ").filter(lambda example: example.get(\"labels\") is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MSZL1Txp7_R",
        "outputId": "0c2fd083-d478-47ed-c8b6-294684005109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [256047, 20545, 256073, 57642, 42365, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [256047, 42365, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "# Check a sample from tokenized data to confirm tokenization\n",
        "print(train_tokenized[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LcPpKA-Op78N",
        "outputId": "9100c830-7908-49c5-b9af-7dd03eb71580"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n{\\n  'input_ids': [...],\\n  'attention_mask': [...],\\n  'labels': [...]\\n}\\n\\n\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The strcture above is interpreted as:\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  'input_ids': [...],\n",
        "  'attention_mask': [...],\n",
        "  'labels': [...]\n",
        "}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSS3vb437RVL",
        "outputId": "12b0e12f-bbf7-4a7d-977b-282ceb7cd464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 18415\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM2dqks9p7zG"
      },
      "source": [
        "## **Fine-Tune NLLB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHFNAuXBp7wZ"
      },
      "source": [
        "### **Configure BitsAndBytes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRktXaopp7tQ"
      },
      "outputs": [],
      "source": [
        "# BitsAndBytes parameters\n",
        "################################################################################\n",
        "use_4bit = True # 4-bit precision on base model loading\n",
        "bnb_4bit_compute_dtype = torch.float16 # compute datatype for 4-bit base model\n",
        "bnb_4bit_quant_type = \"nf4\" # quantization type\n",
        "use_nested_quant = False # activate nested quantization for 4-bit base models (double quantization)\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "aaffa3882b124715989d4981c634a0d0",
            "8a23f3fa6a444c5bac358601f9420997",
            "a13092470c534098964fd40c56f315c5",
            "e7d8f0db93a647699a7f806b7fd82cbd",
            "428124874d6b4fcba86b475386b2674f",
            "4772ad21350242878d9ee7fb371f3f8e",
            "6221274274cc4ac581bf080d57f39cc5",
            "6533d1d3b3284ac09a54472da69c2342",
            "74eeae822176467ca0b801d71b5699cf",
            "69d0ac2680534ecd9b1e20963a861205",
            "857765fc55fb433db12def8f12d64388",
            "4c292a6852c84d299b615b720ac88df9",
            "155c123b6ec443a2854f560b71c9e911",
            "5c630f63a6e843819bed225c49f019ab",
            "713680797506410ba86a4b752063acf8",
            "d0b690c074fc46799825240bcab8affc",
            "77bf1c8808124e5383d42a4bbc4f17a7",
            "2a975adf9e8447c49f50f3f501b335a0",
            "dac768df58c247849d618860924e177b",
            "ee25922e1d52470b9f7f2c9b4fc82a40",
            "48272b35554149e1abf7cf07b518da23",
            "825452c27d954dfdb05710bc64c714db",
            "30aa30031a0d412591193c7282f29af4",
            "3420293b1baa4b1cb774a3de7e4db780",
            "391a4a7225fc4a24a4634a670e461174",
            "b4980bdddf2143fea34131644d9c5011",
            "dea35be0043c40c69406fdc7dfe84f11",
            "f613c54432f742c2a1f7574b124278fe",
            "30a9108ef5014e91b07151f12dd4f55a",
            "09b8807aad62460b98bd60267c051517",
            "04ab4b3eb4c64c56a3272f28c1adb66a",
            "e909a881d5024b33b387eeee172810f0",
            "581d4ac5c8734301a06d78a0ba601c5c",
            "a467c8872fea4e568965f7b4d5c81c28",
            "1a668668000e4671b28e6e2bfdf196cc",
            "dff4d663ffad456abce4248ed512d216",
            "016fe3acc6d24f15866d94a7cb1d2db8",
            "b23873534a634a34814e79ab197b2783",
            "0634240404db4581ad7a20be51bc132e",
            "0fe7dde824074d5fb5561d8e00bfa6b5",
            "cf858da6f0414233b67267d579eca095",
            "fb61f6dda764477ab86f09517b102da9",
            "38e2da0d20404743a0c962679118fdbb",
            "1d80899a0f8d4aefa8247eac4178754b"
          ]
        },
        "id": "Je9N6UmhsCAA",
        "outputId": "5a9f8dac-6810-46f3-a5ec-156c939a3330"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaffa3882b124715989d4981c634a0d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c292a6852c84d299b615b720ac88df9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30aa30031a0d412591193c7282f29af4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a467c8872fea4e568965f7b4d5c81c28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint,\n",
        "                                              device_map=\"auto\",\n",
        "                                              low_cpu_mem_usage=True,  # Explicitly set to avoid the warning\n",
        "                                              quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S4n34njN6AF"
      },
      "source": [
        "### **Test Model with Zero Shot Inferencing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "jhElREp_N5f8",
        "outputId": "49aa8b7d-f0ee-485e-c6ae-c3ea804f06c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¤ English: The weather today is very pleasant.\n",
            "ğŸŒ Igbo: Ihu igwe dá»‹ nná»á» mma taa.\n",
            "CPU times: user 1.11 s, sys: 565 ms, total: 1.67 s\n",
            "Wall time: 1.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# ğŸŒ Define source & target languages (ISO 639-3 codes)\n",
        "src_lang = \"eng_Latn\"\n",
        "tgt_lang = \"ibo_Latn\"\n",
        "\n",
        "# âœï¸ Example input sentence in English\n",
        "input_sentence = \"The weather today is very pleasant.\"\n",
        "\n",
        "# ğŸ”¡ Tokenize with language codes\n",
        "inputs = tokenizer(\n",
        "    input_sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# âœ¨ Set language tokens\n",
        "inputs[\"forced_bos_token_id\"] = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "tokenizer.src_lang = src_lang\n",
        "\n",
        "# ğŸ” Run inference\n",
        "with torch.no_grad():\n",
        "    output_tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "# ğŸ—£ï¸ Decode result\n",
        "translated_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
        "print(f\"ğŸ”¤ English: {input_sentence}\")\n",
        "print(f\"ğŸŒ Igbo: {translated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_ByOm0WsB95"
      },
      "source": [
        "### **Setup Model with LoRA (PEFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFqF8EjfsB7k"
      },
      "outputs": [],
      "source": [
        "# Set up LoRA config with target_modules\n",
        "lora_config = LoraConfig(\n",
        "    r = 8,  # Rank of the decomposition\n",
        "    lora_alpha = 32,  # Scaling factor for LoRA updates\n",
        "    lora_dropout = 0.05,  # Dropout rate for LoRA\n",
        "    task_type = TaskType.SEQ_2_SEQ_LM,  # Sequence-to-sequence task\n",
        "    bias = 'none',\n",
        "    target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers (query, value, key, output)\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters to the model\n",
        "model_with_lora = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wrLctjFsB5V",
        "outputId": "305e1c01-e994-4524-fe31-d9f83ff72166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 616,843,264 || trainable%: 0.2869\n"
          ]
        }
      ],
      "source": [
        "# Check trainable parameters\n",
        "model_with_lora.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYBKR9rJsB2B"
      },
      "source": [
        "### **Prepare DataCollator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7fgOW3EsBzf"
      },
      "outputs": [],
      "source": [
        "# Instantiate the data collator for sequence-to-sequence tasks\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer,\n",
        "                                       model = model_with_lora,\n",
        "                                       padding = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Ps9qrwsBx0"
      },
      "source": [
        "### **Define Seq2SeqTrainingArguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEdtHMzusBvZ"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    eval_strategy = \"epoch\",  # Evaluate after every epoch\n",
        "    logging_dir = f\"{save_dir}/logs\",  # Directory for storing logs\n",
        "    logging_strategy = \"steps\",  # Log every N steps\n",
        "    logging_steps = 25,  # Log every 25 steps\n",
        "    save_strategy = \"epoch\",  # Save model after every epoch\n",
        "    save_total_limit = 3,  # Keep only the latest 3 checkpoints\n",
        "    per_device_train_batch_size = 4,  # Batch size per device for training\n",
        "    per_device_eval_batch_size = 4,  # Batch size per device for evaluation\n",
        "    gradient_accumulation_steps = 2,  # Accumulate gradients for 2 steps before updating weights\n",
        "    num_train_epochs = 3,  # Total number of epochs\n",
        "    predict_with_generate = True,  # Predict with generate\n",
        "    weight_decay = 0.01,  # Weight decay\n",
        "    lr_scheduler_type = \"linear\",  # Linear learning rate scheduler\n",
        "    optim = \"paged_adamw_32bit\",  # Optimizer to use\n",
        "    learning_rate = 2e-5,  # Initial learning rate\n",
        "    eval_steps = 500, # run validation every 500 steps\n",
        "    fp16 = True,  # Use mixed precision training (not recommended for faster training on GPUs, especially A100 GPUs)\n",
        "    load_best_model_at_end = True,  # Load the best model at the end based on evaluation metric\n",
        "    metric_for_best_model = \"eval_loss\",  # Metric to monitor for the best model (e.g., BLEU score for translation)\n",
        "    greater_is_better = False,  # Higher BLEU metric scores are better\n",
        "    report_to = \"none\",  # Use TensorBoard for logging\n",
        "    disable_tqdm = False,  # Enable or disable tqdm (progress bar)\n",
        "    save_steps = 500,  # Save model checkpoints every 500 steps\n",
        "    label_names = [\"labels\"],  # Name of the label column in the dataset\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IOQD149N4S"
      },
      "source": [
        "### **Compute Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzYXaeC49N0m"
      },
      "outputs": [],
      "source": [
        "# metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# def compute_metrics(eval_preds):\n",
        "#     preds, labels = eval_preds\n",
        "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "#     result = metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "#     return {\"bleu\": result[\"score\"]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC7Kcd3e9NyL"
      },
      "source": [
        "### **Training Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oR-8S_C9Nte"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_with_lora,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized.select(range(min(len(val_tokenized), 2000))),\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics  # compute BLEU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW_SqH4u9Nru"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "34MRpJpr9NhC",
        "outputId": "7dc864cf-b8f6-4678-ebd5-d9a659984ba2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training starts at 1747760410.6678948\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4960' max='6906' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4960/6906 41:30 < 16:17, 1.99 it/s, Epoch 2.15/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.228000</td>\n",
              "      <td>7.184656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>7.169800</td>\n",
              "      <td>7.118298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6906' max='6906' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6906/6906 57:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.228000</td>\n",
              "      <td>7.184656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>7.169800</td>\n",
              "      <td>7.118298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>7.114900</td>\n",
              "      <td>7.103433</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ends at 1747763873.3523066\n",
            "Training time: 0h 57m 42s\n"
          ]
        }
      ],
      "source": [
        "# Compute total training time\n",
        "start_time = time.time()\n",
        "print(f\"Training starts at {start_time}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training ends at {end_time}\")\n",
        "\n",
        "total_seconds = end_time - start_time\n",
        "hours = int(total_seconds // 3600)\n",
        "minutes = int((total_seconds % 3600) // 60)\n",
        "seconds = int(total_seconds % 60)\n",
        "\n",
        "print(f\"Training time: {hours}h {minutes}m {seconds}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60KJ7ufX9NfX"
      },
      "source": [
        "## **Save Model and Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-D2ibw39Ndx",
        "outputId": "7628a47c-271f-44d3-cd19-16b87e04cb0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn/En-Ig_FT_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn/En-Ig_FT_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn/En-Ig_FT_model/sentencepiece.bpe.model',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn/En-Ig_FT_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn/En-Ig_FT_model/tokenizer.json')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(f\"{save_dir}/En-Ig_FT_model\")\n",
        "tokenizer.save_pretrained(f\"{save_dir}/En-Ig_FT_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdF8FRyj6h3S"
      },
      "source": [
        "## **Push to Hugging Face**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVx6FxE46uvW",
        "outputId": "76dbc413-6580-43b8-be98-41af39ebb067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yApjs3vN6z7L",
        "outputId": "1a5f7368-d96d-4f3e-f3ae-b8b903b4939a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "The token `DibiaCorp` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `DibiaCorp`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "ff24badb7ddd4cc8af03fdc8a036d533",
            "cf48dd9983ab41be96d5c4f926cc4ba1",
            "6cc122f4b3f842c99801c1b489337f4d",
            "1ed5deb02b124b98b3792ad2cade7c3f",
            "80d89cc7b9524e81a3442061f3ad78c9",
            "ad532dbc41334e0ab7b0068f97a209bf",
            "11a80d59f5904654bc86f609483d079d",
            "bc6f5ddc8ac34f38ba54b58499c9bdd5",
            "e5fa3382ff174d499cbe74fd3cb61238",
            "eb4381497c554309a62b2aee6c6e7b7e",
            "03973a755b6548e48047790e2d546fb2",
            "614b2365d50444faad3e5cb30447c7ba",
            "d2f7df074ca84fd68e057fcbb99aac62",
            "c1c96d00a7db47459b29370a108de52f",
            "45358235cc534984991e4a5476b4ffa7",
            "fe60c4cd2701465e85156ced31c9c9f4",
            "1396a09283f74c51b057757fc68fbbd6",
            "628d0931bff6401d81337440ddf01d06",
            "53b50489e7cf48fa9b8ab852ca0c15a0",
            "0cc276e3095b4bd9af064bdfd597e68c",
            "8d9ec36e64334d6ca19332af054c5386",
            "ebc1862776494f5f914d97e6e334cbef",
            "00303c378abc4308940d6d496a480d63",
            "c11fe8419dc3472dbb7cff7ba67e7fa2",
            "5715e04ded634478af5969df44732b97",
            "769dcfbd87c14c699033aea0939be276",
            "7035049dc3b04ce3ab59b43e81d82c8c",
            "2003f56b8ab9490480685020dc6b1d8f",
            "40bcca3b35fd49ee95170951e92b08cf",
            "e406add622734295989c7245d826e95e",
            "d1c31a81c682472995e64d324966b19c",
            "5913a7ca22d54d0ca550130dcc0e9812",
            "9a83d329e08b4073b389124ebef5ef97",
            "89d8cfb0ca7243d6a42d43387f911143",
            "569fb8e1d8ce4e7193e2333bfeea5b9a",
            "976bb8312f5d4fc283e058fe0aa98fd1",
            "f9af1f8c78bf4fa688da281b42f2ffbb",
            "e1c6c438bff24111b47a79cb8a4fcd40",
            "599092ea2d2b4662a1c9592d984a6e61",
            "2a2d9d7db75c4c338573f97000f6041a",
            "4a312233a37c4565a2857009726875d3",
            "0ece0ea68d4b43008503fec2fd4dff6d",
            "ac04b9dea0fa41da8ace958bce0a8abd",
            "5b1e1919c1374f6da32eeb2dd99500df",
            "c367b27a278b4ec39a08711ad1513a2b",
            "39a5a3f23cf743fda54569c00b6ae903",
            "5da0e376cbc341e1b08fa2caa6e55d77",
            "e07dad0d3ab54781b8e79f709bae32ba",
            "73fc8ef1cd0442d7aee653faa39da1c6",
            "6a6c00765cac4720a0c9cb7a15938195",
            "6144be731cdf47d7bb7bd37817a6ba51",
            "56bfa52b1af24ca1aa6f6721682f102a",
            "b3aabc39ac7e457abc0e485842838998",
            "6042d336210e46cebf92510812f0e5de",
            "fc2c50352795407a86cc42411e55377f",
            "142ec0fda53149af89ce5d0ab2a7a542",
            "8fe84c348e70447d8a39c233746550ee",
            "422a0c5b4a894498989f9ada602e7a6d",
            "c124e6caf9ca49bbaf02741114f79d9e",
            "b61f17fa90a8498b97e7e71049091edf",
            "d77aa72ca26d4b6495d83b540099805e",
            "6de13e835244413f827fc6a8d9bb121c",
            "cd74707d34bb450aab48dce510364c51",
            "2fd1caca789e496e9d11dc4d2fd89460",
            "3fb7e5dee75a47d18af806eb0070a37e",
            "65a88c7982e34dfd800cbf70d7cc0a4d",
            "84437bf6ed6b447cb0f58d277052593d",
            "8ba1c4f8367d4e00afaf923ea0bf967f",
            "6b68af4f571f4505910ec95a756da9fb",
            "2ec840d4a60148e6a4de6ff3073374a0",
            "ab1bef5ca00745c68e56c2fec996102b",
            "6e6ab2f9c81e41ecbfd53fec19de093e",
            "7d160265b3444c24bbbb7778359d7a24",
            "0db868f533ff47c6b1d0d9b2f11425d8",
            "53fd3ec26c3a4db6bd3bc3e0c242e1fe",
            "5c053fdc81b94af2ba2b77f1a281a55f",
            "f025ae5c81764ccebda991bdeaec09f9",
            "d94a679c5422417690136ddc4fdd8437",
            "b22527903f7e4266b375d7c5613bde05",
            "0a6a118335fa43b086e5fc9c9f468741",
            "b742e4d49695438fac698dd52c2d5614",
            "9577998ea63d422396f2d727af947e4a",
            "311aa7207bef4a18b1ad550ff4d0b9bf",
            "9e098ecc441341638948a8ef68103cae",
            "538c79d74aad4349a2430a0feab96b02",
            "66c6171a17f840ca975c76be125e7ab4",
            "92b2c95c24a846fbb58bd43cfa3a3e55",
            "86b62fdf6494441799df714cc3c567b1",
            "298692d11e254c0f991d04c0f0bda7d3",
            "58b12c581aea46f3bea9f165c02d396b",
            "3f051e240a254ded992375263b0a8c78",
            "6e7c7b4108da466b898de6f95d66fe08",
            "7b7d019a7f764bf6bdccbcf8df0f201f",
            "914d5d5ee87e4590b5e370059aa14c1e",
            "423d3c3686dd4fdf943017d54b18ecc3",
            "ce8f1a1618c14009a92dda590faa22bc",
            "80b28222f58247169d36ac98c1842d1f",
            "d325d02d464a47f381cc34f65c657381",
            "d823d5f8a6254c32968d8354417a2624"
          ]
        },
        "id": "hNPGgDX56z-w",
        "outputId": "87ca53e9-20c7-4746-b913-6e51dc73ec3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff24badb7ddd4cc8af03fdc8a036d533",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "614b2365d50444faad3e5cb30447c7ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00303c378abc4308940d6d496a480d63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89d8cfb0ca7243d6a42d43387f911143",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c367b27a278b4ec39a08711ad1513a2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/7.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "142ec0fda53149af89ce5d0ab2a7a542",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84437bf6ed6b447cb0f58d277052593d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d94a679c5422417690136ddc4fdd8437",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "298692d11e254c0f991d04c0f0bda7d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/32.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/drakensberg85/English-Igbo_NLLB_FT_model/commit/eb3f4c25c6f66e2f0ac8a4745e3d760e58a55966', commit_message='Upload tokenizer', commit_description='', oid='eb3f4c25c6f66e2f0ac8a4745e3d760e58a55966', pr_url=None, repo_url=RepoUrl('https://huggingface.co/drakensberg85/English-Igbo_NLLB_FT_model', endpoint='https://huggingface.co', repo_type='model', repo_id='drakensberg85/English-Igbo_NLLB_FT_model'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Define repo details\n",
        "repo_name = \"drakensberg85/English-Igbo_NLLB_FT_model\"\n",
        "model_path = f\"{save_dir}/En-Ig_FT_model\"\n",
        "\n",
        "# Upload using transformers\n",
        "AutoModelForSeq2SeqLM.from_pretrained(model_path).push_to_hub(repo_name)\n",
        "AutoTokenizer.from_pretrained(model_path).push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI3p6I-d9NZH"
      },
      "source": [
        "## **TensorBoard Logging and Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcKoftke9NXZ"
      },
      "outputs": [],
      "source": [
        "# writer = SummaryWriter(f\"{save_dir}/logs\")\n",
        "# print(\"Training complete. View metrics using TensorBoard:\")\n",
        "# print(f\"Run this in Colab terminal: tensorboard --logdir={save_dir}/logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZQqrrLb9NTd"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir \"{/content/drive/MyDrive/Colab Notebooks/NLLB_200/En-Ig_LaTn/}logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igDlKhBK9NQb"
      },
      "source": [
        "## **Inference Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdRXyhpQw_-i",
        "outputId": "b0cd9127-ff12-49c9-9516-751f4356afa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: human\n",
            "Igbo: Mmadá»¥\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Use the correct model path\n",
        "model_path = f\"{save_dir}/En-Ig_FT_model\"\n",
        "\n",
        "# Load tokenizer and model from local files\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# Send model to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Example sentence for translation\n",
        "english_sentence = \"human\"\n",
        "source_sentence = f\">>ibo_Latn<< {english_sentence}\"\n",
        "\n",
        "# Tokenize input and move to device\n",
        "inputs = tokenizer(source_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate translation\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)\n",
        "\n",
        "# Decode and print translation\n",
        "igbo_translation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"English: {english_sentence}\")\n",
        "print(f\"Igbo: {igbo_translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spj8CD8Uw_7b"
      },
      "source": [
        "## **Incorporating Chainlit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBI_xyl1w_4g",
        "outputId": "b49e84ad-73e6-4e45-ead6-dae5d608a716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import chainlit as cl\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_path = f\"{save_dir}/En-Ig_FT_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    await cl.Message(content=\"ğŸ‘‹ Welcome! Type something in English and I'll translate it to Igbo!\").send()\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    input_text = f\">>ibo_Latn<< {message.content}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Create an empty Chainlit message to stream into\n",
        "    response = cl.Message(content=\"\")\n",
        "    await response.send()\n",
        "\n",
        "    # Generate tokens step-by-step\n",
        "    output_tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=1,  # Beam search disables streaming behavior\n",
        "        do_sample=False,\n",
        "        output_scores=False,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "    # Stream tokens (you can simulate streaming with a short delay per chunk if needed)\n",
        "    output_text = tokenizer.decode(output_tokens.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Simulate streaming (token-by-token)\n",
        "    for token in output_text.split():\n",
        "        response.content += token + \" \"\n",
        "        await response.update()\n",
        "\n",
        "    # Final update\n",
        "    await response.update()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mycaZysfw_o4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}